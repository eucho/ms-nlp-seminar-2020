{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "63de4277-c079-1630-2e47-44d138f3e7bf"
   },
   "source": [
    "# Library Import #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "_cell_guid": "193a91a4-7404-a01b-2c78-6b7385979b9c"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "english_stemmer=nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "from sklearn.feature_selection.univariate_selection import SelectKBest, chi2, f_classif\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.linear_model import SGDClassifier, SGDRegressor,LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "import sys\n",
    "import os, glob\n",
    "import argparse\n",
    "from sklearn.pipeline import Pipeline\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import six\n",
    "from abc import ABCMeta\n",
    "from scipy import sparse\n",
    "from scipy.sparse import issparse\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils import check_X_y, check_array\n",
    "from sklearn.utils.extmath import safe_sparse_dot\n",
    "from sklearn.preprocessing import normalize, binarize, LabelBinarizer\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Lambda\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM, SimpleRNN, GRU\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from collections import defaultdict\n",
    "from keras.layers.convolutional import Convolution1D\n",
    "from keras import backend as K\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f98318d0-c122-24a1-a9a6-62ab93985428"
   },
   "source": [
    "# Data Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c39b53ac-2080-bfb4-124d-5e775af2cec2"
   },
   "source": [
    "we'll use all of the dates up to the end of 2014 as our training data and everything after as testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "_cell_guid": "28217d5a-e580-fa92-fdc2-45a4842812e0"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('./input/Combined_News_DJIA.csv')\n",
    "tweets = pd.read_csv('./input/realdonaldtrump.csv')\n",
    "tweets[\"Date\"] = pd.to_datetime(tweets[\"Date\"])\n",
    "tweets['Date']=tweets['Date'].dt.strftime('%Y-%m-%d')\n",
    "tweets = tweets.groupby('Date')['Tweet'].apply(', '.join).reset_index()\n",
    "tweets['Top26'] = tweets['Tweet'].str.replace('http\\S+|www.\\S+', ' ', case=False)\n",
    "tweets.to_csv('./output/realdonaldtrump-cooked.csv')\n",
    "tweets.drop(columns=[\"Tweet\"], inplace=True)\n",
    "data = pd.merge(tweets, data,how='left',on='Date')\n",
    "data.drop(columns=[\"Label\"], inplace=True)\n",
    "\n",
    "#add more labels\n",
    "more_labels = pd.read_csv('./input/DJIA_table_all_addlabel.csv')\n",
    "# binary label\n",
    "#more_labels['Label']=np.where(more_labels['Open'] < more_labels['Close'], 1.0, 0.0)\n",
    "# label_4part\n",
    "more_labels['Label'] = more_labels['Label_4part']\n",
    "binaryLabel = more_labels[['Date', 'Label', 'Label_4part', 'Label_3part']]\n",
    "binaryLabel[\"Date\"] = pd.to_datetime(binaryLabel[\"Date\"])\n",
    "binaryLabel['Date']=binaryLabel['Date'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "data = pd.merge(data, binaryLabel,how='left',on='Date')\n",
    "data.head(10)\n",
    "\n",
    "# Drop rows with any empty cells\n",
    "data.dropna(\n",
    "    axis=0,\n",
    "    how='any',\n",
    "    thresh=None,\n",
    "    subset=None,\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "#train = data[data['Date'] < '2015-05-31']\n",
    "#test = data[data['Date'] > '2015-06-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add Trump tweet\n",
    "tweets = pd.read_csv('./input/realdonaldtrump.csv')\n",
    "tweets = tweets.groupby('Date')['Tweet'].apply(', '.join).reset_index()\n",
    "#tweets.head(20)\n",
    "#remove URLs in tweet\n",
    "tweets[\"Date\"] = pd.to_datetime(tweets[\"Date\"])\n",
    "tweets[\"Date\"]=tweets[\"Date\"].dt.strftime('%Y-%m-%d')\n",
    "tweets['Tweet'] = tweets['Tweet'].str.replace('http\\S+|www.\\S+', ' ', case=False)\n",
    "tweets['Tweet1'] = tweets['Tweet']\n",
    "tweets.head(10)\n",
    "tweets.to_csv('./output/realdonaldtrump-cooked.csv')\n",
    "\n",
    "tweets = pd.merge(tweets, binaryLabel[['Date','Label', 'Label_3part','Label_4part']],how='left',on='Date')\n",
    "# Drop rows with any empty cells\n",
    "tweets.dropna(\n",
    "    axis=0,\n",
    "    how='any',\n",
    "    thresh=None,\n",
    "    subset=None,\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "train = tweets[tweets['Date'] < '2019-05-31']\n",
    "test = tweets[tweets['Date'] > '2019-06-01']\n",
    "\n",
    "#tweets.drop(columns=[\"Tweet\"], inplace=True)\n",
    "#tweetsTrain = tweets[tweets['Date'] < '2015-05-31']\n",
    "#tweetsTest = tweets[tweets['Date'] > '2015-06-01']\n",
    "\n",
    "#train = pd.merge(train, tweetsTrain[['Date','Top26']],how='left',on='Date')\n",
    "#test = pd.merge(test, tweetsTest[['Date','Top26']],how='left',on='Date')\n",
    "#data.to_csv('./output/combined-tweets.csv')\n",
    "#data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3d372c26-3c4c-7890-3089-c8ddb17ca81a"
   },
   "source": [
    "# Data Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b43212e0-9344-a216-b86b-37cfbfb37752"
   },
   "source": [
    "First, we transform the string of news into the  number of words as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "_cell_guid": "a87f0d3d-80f3-af90-9368-dedde8c1154f"
   },
   "outputs": [],
   "source": [
    "trainheadlines = []\n",
    "for row in range(0,len(train.index)):\n",
    "    trainheadlines.append(' '.join(str(x) for x in train.iloc[row,1:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "_cell_guid": "66913298-12f3-da89-030a-90ba38b1e11b"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(2195, 31549)\n"
     ]
    }
   ],
   "source": [
    "basicvectorizer = CountVectorizer()\n",
    "basictrain = basicvectorizer.fit_transform(trainheadlines)\n",
    "print(basictrain.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4647ab33-fd7e-0d98-9afd-7ea21eb02cf8"
   },
   "source": [
    "## Logic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ee566614-2f26-df99-3c54-700a07cf83bd"
   },
   "source": [
    "### Logic Regression 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "dc0ac1c7-f4fe-3bbe-18fc-f578947626b1"
   },
   "source": [
    "Algorithm: Logic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5a6e98cc-ec7f-3dd2-5dce-2c2c04cff260"
   },
   "source": [
    "Input: the counts of single words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "_cell_guid": "5adfb2da-7467-505b-d44b-0a028e60227b"
   },
   "outputs": [],
   "source": [
    "basicmodel = LogisticRegression()\n",
    "basicmodel = basicmodel.fit(basictrain, train[\"Label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "_cell_guid": "56d1ea73-641b-6fdf-698f-32518fe99dec"
   },
   "outputs": [],
   "source": [
    "testheadlines = []\n",
    "for row in range(0,len(test.index)):\n",
    "    testheadlines.append(' '.join(str(x) for x in test.iloc[row,1:2]))\n",
    "basictest = basicvectorizer.transform(testheadlines)\n",
    "preds1 = basicmodel.predict(basictest)\n",
    "acc1=accuracy_score(test['Label'], preds1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "_cell_guid": "bc13f327-70bd-2f8f-4ab7-6a9dfae356e9"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Logic Regression 1 accuracy:  0.3840304182509506\n"
     ]
    }
   ],
   "source": [
    "print('Logic Regression 1 accuracy: ',acc1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c08a6f88-d723-97b5-dd09-a61b1f3f7829"
   },
   "source": [
    "The accuracy is only 0.42.\n",
    "\n",
    "Trump tweets as only source\n",
    "0.51 accuracy if we use 2019/05/31 train/test data\n",
    "0.49 accuracy if we use 2018/05/31 train/test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "_cell_guid": "2303559d-4ad8-7d30-ff3d-ee3bb91440ae"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "            Word  Coefficient\n",
       "12662       help     0.651388\n",
       "27014       sure     0.488409\n",
       "30299      waste     0.481729\n",
       "10250  fantastic     0.469194\n",
       "6086        club     0.460507"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Word</th>\n      <th>Coefficient</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>12662</th>\n      <td>help</td>\n      <td>0.651388</td>\n    </tr>\n    <tr>\n      <th>27014</th>\n      <td>sure</td>\n      <td>0.488409</td>\n    </tr>\n    <tr>\n      <th>30299</th>\n      <td>waste</td>\n      <td>0.481729</td>\n    </tr>\n    <tr>\n      <th>10250</th>\n      <td>fantastic</td>\n      <td>0.469194</td>\n    </tr>\n    <tr>\n      <th>6086</th>\n      <td>club</td>\n      <td>0.460507</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 344
    }
   ],
   "source": [
    "basicwords = basicvectorizer.get_feature_names()\n",
    "basiccoeffs = basicmodel.coef_.tolist()[0]\n",
    "coeffdf = pd.DataFrame({'Word' : basicwords, \n",
    "                        'Coefficient' : basiccoeffs})\n",
    "coeffdf = coeffdf.sort_values(['Coefficient', 'Word'], ascending=[0, 1])\n",
    "coeffdf.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "_cell_guid": "949e33a5-f63d-0059-2ee0-17e625aede36"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "               Word  Coefficient\n",
       "21942  presidential    -0.386446\n",
       "20150     obamacare    -0.399412\n",
       "20113       nytimes    -0.413369\n",
       "26876       success    -0.450378\n",
       "23018        record    -0.536661"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Word</th>\n      <th>Coefficient</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>21942</th>\n      <td>presidential</td>\n      <td>-0.386446</td>\n    </tr>\n    <tr>\n      <th>20150</th>\n      <td>obamacare</td>\n      <td>-0.399412</td>\n    </tr>\n    <tr>\n      <th>20113</th>\n      <td>nytimes</td>\n      <td>-0.413369</td>\n    </tr>\n    <tr>\n      <th>26876</th>\n      <td>success</td>\n      <td>-0.450378</td>\n    </tr>\n    <tr>\n      <th>23018</th>\n      <td>record</td>\n      <td>-0.536661</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 345
    }
   ],
   "source": [
    "coeffdf.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6aa2253b-0cf6-4853-4ff5-6143dae71fb1"
   },
   "source": [
    "### Logic Regression 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "65239831-43f2-290b-8e21-b4a9f82e5a54"
   },
   "source": [
    "Algorithm: Logic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a108baea-371d-d93f-7d65-213ecd4337ec"
   },
   "source": [
    "Input: the counts of phrases with two connected words(exclude words which are too common like \"a\" ,\"an\" ,\"the\" and words too uncommon of which counts are too small )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "15df5336-f6f9-cd66-b020-6072aff3083b"
   },
   "source": [
    "We delete phrases of which frequency lower than 0.03 or higher than 0.97"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "_cell_guid": "dbdf7b4f-4673-49f1-d510-377026c95611"
   },
   "outputs": [],
   "source": [
    "advancedvectorizer = TfidfVectorizer( min_df=0.03, max_df=0.97, max_features = 200000, ngram_range = (2, 2))\n",
    "advancedtrain = advancedvectorizer.fit_transform(trainheadlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "_cell_guid": "b6bf747c-021c-eb7f-cd50-2240138ed555"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(2195, 412)\n"
     ]
    }
   ],
   "source": [
    "print(advancedtrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "_cell_guid": "8c225d22-d997-5db8-877e-03022a0785b4"
   },
   "outputs": [],
   "source": [
    "advancedmodel = LogisticRegression()\n",
    "advancedmodel = advancedmodel.fit(advancedtrain, train[\"Label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "_cell_guid": "1b3421b9-b326-4ef5-4235-2e764e5cd3a3"
   },
   "outputs": [],
   "source": [
    "testheadlines = []\n",
    "for row in range(0,len(test.index)):\n",
    "    testheadlines.append(' '.join(str(x) for x in test.iloc[row,1:2]))\n",
    "advancedtest = advancedvectorizer.transform(testheadlines)\n",
    "preds2 = advancedmodel.predict(advancedtest)\n",
    "acc2=accuracy_score(test['Label'], preds2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "_cell_guid": "422fb025-f985-e010-3e9e-2ee8c4494c3e"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Logic Regression 2 accuracy:  0.3574144486692015\n"
     ]
    }
   ],
   "source": [
    "print('Logic Regression 2 accuracy: ', acc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8f5a4ca4-75a0-6a24-2c34-61855d2b102a"
   },
   "source": [
    "The accuracy is higher than input of single words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "_cell_guid": "3af2a0fb-a8ef-84f2-2a80-3568d327baec"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "             Words  Coefficient\n",
       "281  the president     1.040487\n",
       "272       the last     1.010511\n",
       "324        to help     0.954562\n",
       "121          in my     0.934071\n",
       "327          to me     0.917819"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Words</th>\n      <th>Coefficient</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>281</th>\n      <td>the president</td>\n      <td>1.040487</td>\n    </tr>\n    <tr>\n      <th>272</th>\n      <td>the last</td>\n      <td>1.010511</td>\n    </tr>\n    <tr>\n      <th>324</th>\n      <td>to help</td>\n      <td>0.954562</td>\n    </tr>\n    <tr>\n      <th>121</th>\n      <td>in my</td>\n      <td>0.934071</td>\n    </tr>\n    <tr>\n      <th>327</th>\n      <td>to me</td>\n      <td>0.917819</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 351
    }
   ],
   "source": [
    "advwords = advancedvectorizer.get_feature_names()\n",
    "advcoeffs = advancedmodel.coef_.tolist()[0]\n",
    "advcoeffdf = pd.DataFrame({'Words' : advwords, \n",
    "                        'Coefficient' : advcoeffs})\n",
    "advcoeffdf = advcoeffdf.sort_values(['Coefficient', 'Words'], ascending=[0, 1])\n",
    "advcoeffdf.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "_cell_guid": "b1fd8c2b-23fd-7dda-cf8d-0381c3d0736f"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "               Words  Coefficient\n",
       "339          to work    -0.777660\n",
       "197           on his    -0.800374\n",
       "277         the next    -0.800603\n",
       "149          join me    -0.824053\n",
       "109  hillary clinton    -0.843789"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Words</th>\n      <th>Coefficient</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>339</th>\n      <td>to work</td>\n      <td>-0.777660</td>\n    </tr>\n    <tr>\n      <th>197</th>\n      <td>on his</td>\n      <td>-0.800374</td>\n    </tr>\n    <tr>\n      <th>277</th>\n      <td>the next</td>\n      <td>-0.800603</td>\n    </tr>\n    <tr>\n      <th>149</th>\n      <td>join me</td>\n      <td>-0.824053</td>\n    </tr>\n    <tr>\n      <th>109</th>\n      <td>hillary clinton</td>\n      <td>-0.843789</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 352
    }
   ],
   "source": [
    "advcoeffdf.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "39b0d7a4-cedc-d51c-11f4-d3a1d454682f"
   },
   "source": [
    "### Logic Regression 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4cf79ce1-930d-474a-f07b-dd397f9a72eb"
   },
   "source": [
    "Algorithm: Logic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a62cefba-3f14-ce21-49a5-1fdd09556dde"
   },
   "source": [
    "Input: the counts of phrases with three connected words(exclude words which are too common like \"a\" ,\"an\" ,\"the\" and words too uncommon of which counts are too small )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "_cell_guid": "886f33fe-d47f-d545-1b73-29f1e3618e01"
   },
   "outputs": [],
   "source": [
    "advancedvectorizer = TfidfVectorizer( min_df=0.0039, max_df=0.1, max_features = 200000, ngram_range = (3, 3))\n",
    "advancedtrain = advancedvectorizer.fit_transform(trainheadlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "_cell_guid": "5437284a-b14e-96cd-e6d5-97c41790d42a"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(2195, 1981)\n"
     ]
    }
   ],
   "source": [
    "print(advancedtrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "_cell_guid": "5e29bfca-0041-5b86-fdff-9c502f4aa1ad"
   },
   "outputs": [],
   "source": [
    "advancedmodel = LogisticRegression()\n",
    "advancedmodel = advancedmodel.fit(advancedtrain, train[\"Label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {
    "_cell_guid": "d5717cfb-3f8c-1af4-665b-6a22e6e9c9ba"
   },
   "outputs": [],
   "source": [
    "testheadlines = []\n",
    "for row in range(0,len(test.index)):\n",
    "    testheadlines.append(' '.join(str(x) for x in test.iloc[row,1:2]))\n",
    "advancedtest = advancedvectorizer.transform(testheadlines)\n",
    "preds3 = advancedmodel.predict(advancedtest)\n",
    "acc3 = accuracy_score(test['Label'], preds3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {
    "_cell_guid": "b700896c-936b-004d-2d70-50fefee2129c"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Logic Regression 3 accuracy:  0.35361216730038025\n"
     ]
    }
   ],
   "source": [
    "print('Logic Regression 3 accuracy: ', acc3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2a438254-8acb-4db7-4f38-bb07be962dc2"
   },
   "source": [
    "The accuracy is lower than input of phrases with two connected words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {
    "_cell_guid": "66fee024-03b4-5964-f925-cfb3746a4741"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                  Words  Coefficient\n",
       "877         my new book     1.063642\n",
       "895   never should have     0.937456\n",
       "785        listen to my     0.881427\n",
       "794  looking forward to     0.871569\n",
       "643  in today trumpvlog     0.859418"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Words</th>\n      <th>Coefficient</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>877</th>\n      <td>my new book</td>\n      <td>1.063642</td>\n    </tr>\n    <tr>\n      <th>895</th>\n      <td>never should have</td>\n      <td>0.937456</td>\n    </tr>\n    <tr>\n      <th>785</th>\n      <td>listen to my</td>\n      <td>0.881427</td>\n    </tr>\n    <tr>\n      <th>794</th>\n      <td>looking forward to</td>\n      <td>0.871569</td>\n    </tr>\n    <tr>\n      <th>643</th>\n      <td>in today trumpvlog</td>\n      <td>0.859418</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 358
    }
   ],
   "source": [
    "advwords = advancedvectorizer.get_feature_names()\n",
    "advcoeffs = advancedmodel.coef_.tolist()[0]\n",
    "advcoeffdf = pd.DataFrame({'Words' : advwords, \n",
    "                        'Coefficient' : advcoeffs})\n",
    "advcoeffdf = advcoeffdf.sort_values(['Coefficient', 'Words'], ascending=[0, 1])\n",
    "advcoeffdf.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {
    "_cell_guid": "c50f97d8-a6f7-0a60-9edf-42bb8b7aa492"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                        Words  Coefficient\n",
       "1491   think like billionaire    -0.543612\n",
       "1357      the failing nytimes    -0.556536\n",
       "1767              we need you    -0.593958\n",
       "277   crooked hillary clinton    -0.659938\n",
       "746                join me in    -0.750222"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Words</th>\n      <th>Coefficient</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1491</th>\n      <td>think like billionaire</td>\n      <td>-0.543612</td>\n    </tr>\n    <tr>\n      <th>1357</th>\n      <td>the failing nytimes</td>\n      <td>-0.556536</td>\n    </tr>\n    <tr>\n      <th>1767</th>\n      <td>we need you</td>\n      <td>-0.593958</td>\n    </tr>\n    <tr>\n      <th>277</th>\n      <td>crooked hillary clinton</td>\n      <td>-0.659938</td>\n    </tr>\n    <tr>\n      <th>746</th>\n      <td>join me in</td>\n      <td>-0.750222</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 359
    }
   ],
   "source": [
    "advcoeffdf.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a4229efa-49d5-82fb-5a1d-e1ca4001e0ac"
   },
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5ccc2e71-28c6-f425-f229-f1395293bffc"
   },
   "source": [
    "### NBayes 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "_cell_guid": "476ba54c-4b47-6029-e0de-29e6fcb64845"
   },
   "outputs": [],
   "source": [
    "advancedvectorizer = TfidfVectorizer( min_df=0.1, max_df=0.7, max_features = 200000, ngram_range = (1, 1))\n",
    "advancedtrain = advancedvectorizer.fit_transform(trainheadlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {
    "_cell_guid": "1ce37cf1-5c09-9fe7-09ec-a23d6d3b749e"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(2195, 242)\n"
     ]
    }
   ],
   "source": [
    "print(advancedtrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {
    "_cell_guid": "4726d110-1821-ea71-dc5c-7a21ad628dd1"
   },
   "outputs": [],
   "source": [
    "advancedmodel = MultinomialNB(alpha=0.01)\n",
    "advancedmodel = advancedmodel.fit(advancedtrain, train[\"Label\"])\n",
    "testheadlines = []\n",
    "for row in range(0,len(test.index)):\n",
    "    testheadlines.append(' '.join(str(x) for x in test.iloc[row,1:2]))\n",
    "advancedtest = advancedvectorizer.transform(testheadlines)\n",
    "preds4 = advancedmodel.predict(advancedtest)\n",
    "acc4=accuracy_score(test['Label'], preds4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "_cell_guid": "ff1f469d-1c12-5a3d-1aed-148179c26824"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "NBayes 1 accuracy:  0.4296577946768061\n"
     ]
    }
   ],
   "source": [
    "print('NBayes 1 accuracy: ', acc4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "_cell_guid": "02a9958e-ab18-bf74-58f1-160c5b47cbff"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "               Words  Coefficient\n",
       "161  realdonaldtrump    -4.039677\n",
       "240              you    -4.087514\n",
       "204            trump    -4.127277\n",
       "80             great    -4.389234\n",
       "230             with    -4.400817"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Words</th>\n      <th>Coefficient</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>161</th>\n      <td>realdonaldtrump</td>\n      <td>-4.039677</td>\n    </tr>\n    <tr>\n      <th>240</th>\n      <td>you</td>\n      <td>-4.087514</td>\n    </tr>\n    <tr>\n      <th>204</th>\n      <td>trump</td>\n      <td>-4.127277</td>\n    </tr>\n    <tr>\n      <th>80</th>\n      <td>great</td>\n      <td>-4.389234</td>\n    </tr>\n    <tr>\n      <th>230</th>\n      <td>with</td>\n      <td>-4.400817</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 364
    }
   ],
   "source": [
    "advwords = advancedvectorizer.get_feature_names()\n",
    "advcoeffs = advancedmodel.coef_.tolist()[0]\n",
    "advcoeffdf = pd.DataFrame({'Words' : advwords, \n",
    "                        'Coefficient' : advcoeffs})\n",
    "advcoeffdf = advcoeffdf.sort_values(['Coefficient', 'Words'], ascending=[0, 1])\n",
    "advcoeffdf.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "_cell_guid": "6223b466-48eb-0a0a-5d36-c082e50b1fe5"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       Words  Coefficient\n",
       "72   getting    -6.763486\n",
       "51     doesn    -6.839422\n",
       "163   record    -6.849322\n",
       "180  success    -6.873683\n",
       "172    smart    -7.047674"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Words</th>\n      <th>Coefficient</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>72</th>\n      <td>getting</td>\n      <td>-6.763486</td>\n    </tr>\n    <tr>\n      <th>51</th>\n      <td>doesn</td>\n      <td>-6.839422</td>\n    </tr>\n    <tr>\n      <th>163</th>\n      <td>record</td>\n      <td>-6.849322</td>\n    </tr>\n    <tr>\n      <th>180</th>\n      <td>success</td>\n      <td>-6.873683</td>\n    </tr>\n    <tr>\n      <th>172</th>\n      <td>smart</td>\n      <td>-7.047674</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 365
    }
   ],
   "source": [
    "advcoeffdf.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "dcc814d2-014d-5cb9-b106-52ff648d9c18"
   },
   "source": [
    "### NBayes 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "_cell_guid": "dfe60202-6348-d724-c860-c3b46c66c447"
   },
   "outputs": [],
   "source": [
    "advancedvectorizer = TfidfVectorizer( min_df=0.03, max_df=0.2, max_features = 200000, ngram_range = (2, 2))\n",
    "advancedtrain = advancedvectorizer.fit_transform(trainheadlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {
    "_cell_guid": "28b21c32-28de-1bc4-161e-dcf63737caca"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(2195, 398)\n"
     ]
    }
   ],
   "source": [
    "print(advancedtrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "_cell_guid": "23ab2ed6-8e9a-4291-2f9b-6fc2563f33ca"
   },
   "outputs": [],
   "source": [
    "advancedmodel = MultinomialNB(alpha=0.0001)\n",
    "advancedmodel = advancedmodel.fit(advancedtrain, train[\"Label\"])\n",
    "testheadlines = []\n",
    "for row in range(0,len(test.index)):\n",
    "    testheadlines.append(' '.join(str(x) for x in test.iloc[row,1:2]))\n",
    "advancedtest = advancedvectorizer.transform(testheadlines)\n",
    "preds5 = advancedmodel.predict(advancedtest)\n",
    "acc5 = accuracy_score(test['Label'], preds5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {
    "_cell_guid": "aa7d48c3-60cc-0895-7ed3-ddbf796339ff"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "NBayes 2 accuracy:  0.39923954372623577\n"
     ]
    }
   ],
   "source": [
    "print('NBayes 2 accuracy: ', acc5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {
    "_cell_guid": "f49cb3c4-b80b-48a8-69be-ac6c579e2ed4"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "             Words  Coefficient\n",
       "145     last night    -4.679193\n",
       "72      forward to    -4.779096\n",
       "18         and the    -4.784708\n",
       "68   for president    -4.910518\n",
       "167   my interview    -4.934958"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Words</th>\n      <th>Coefficient</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>145</th>\n      <td>last night</td>\n      <td>-4.679193</td>\n    </tr>\n    <tr>\n      <th>72</th>\n      <td>forward to</td>\n      <td>-4.779096</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>and the</td>\n      <td>-4.784708</td>\n    </tr>\n    <tr>\n      <th>68</th>\n      <td>for president</td>\n      <td>-4.910518</td>\n    </tr>\n    <tr>\n      <th>167</th>\n      <td>my interview</td>\n      <td>-4.934958</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 370
    }
   ],
   "source": [
    "advwords = advancedvectorizer.get_feature_names()\n",
    "advcoeffs = advancedmodel.coef_.tolist()[0]\n",
    "advcoeffdf = pd.DataFrame({'Words' : advwords, \n",
    "                        'Coefficient' : advcoeffs})\n",
    "advcoeffdf = advcoeffdf.sort_values(['Coefficient', 'Words'], ascending=[0, 1])\n",
    "advcoeffdf.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {
    "_cell_guid": "087f9b18-c50b-17b5-60b6-2cf2dada7c3c"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "        Words  Coefficient\n",
       "95    has the    -7.915595\n",
       "66     for me    -8.101348\n",
       "386   you don    -8.131665\n",
       "108  how much    -8.671412\n",
       "361  what the    -8.854786"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Words</th>\n      <th>Coefficient</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>95</th>\n      <td>has the</td>\n      <td>-7.915595</td>\n    </tr>\n    <tr>\n      <th>66</th>\n      <td>for me</td>\n      <td>-8.101348</td>\n    </tr>\n    <tr>\n      <th>386</th>\n      <td>you don</td>\n      <td>-8.131665</td>\n    </tr>\n    <tr>\n      <th>108</th>\n      <td>how much</td>\n      <td>-8.671412</td>\n    </tr>\n    <tr>\n      <th>361</th>\n      <td>what the</td>\n      <td>-8.854786</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 371
    }
   ],
   "source": [
    "advcoeffdf.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4bfcc58e-1ed4-18d5-a321-e58a4744dd8c"
   },
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a77b2afd-5654-aaff-74a9-0013fe4e8164"
   },
   "source": [
    "### RF 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {
    "_cell_guid": "91fe0b03-37a0-eb19-2632-cdc04067b12f"
   },
   "outputs": [],
   "source": [
    "advancedvectorizer = TfidfVectorizer( min_df=0.01, max_df=0.99, max_features = 200000, ngram_range = (1, 1))\n",
    "advancedtrain = advancedvectorizer.fit_transform(trainheadlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {
    "_cell_guid": "878756f6-f57d-88cb-fe82-5181c1e40901"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(2195, 2144)\n"
     ]
    }
   ],
   "source": [
    "print(advancedtrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {
    "_cell_guid": "b561af90-db7d-ef58-26a2-5721d5432615"
   },
   "outputs": [],
   "source": [
    "advancedmodel = RandomForestClassifier()\n",
    "advancedmodel = advancedmodel.fit(advancedtrain, train[\"Label\"])\n",
    "testheadlines = []\n",
    "for row in range(0,len(test.index)):\n",
    "    testheadlines.append(' '.join(str(x) for x in test.iloc[row,1:2]))\n",
    "advancedtest = advancedvectorizer.transform(testheadlines)\n",
    "preds6 = advancedmodel.predict(advancedtest)\n",
    "acc6 = accuracy_score(test['Label'], preds6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {
    "_cell_guid": "91fe3c5f-b005-97f0-da7c-597c29521877"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "RF 1 accuracy:  0.3916349809885932\n"
     ]
    }
   ],
   "source": [
    "print('RF 1 accuracy: ', acc6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "76a971f3-878d-33e2-249a-a2b063aa5460"
   },
   "source": [
    "### RF 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {
    "_cell_guid": "1db5461b-1ee9-2196-6725-66a6c2376a19"
   },
   "outputs": [],
   "source": [
    "advancedvectorizer = TfidfVectorizer( min_df=0.03, max_df=0.2, max_features = 200000, ngram_range = (2, 2))\n",
    "advancedtrain = advancedvectorizer.fit_transform(trainheadlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "_cell_guid": "e1fb8019-c981-9d18-8f54-2750387ec071"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(2195, 398)\n"
     ]
    }
   ],
   "source": [
    "print(advancedtrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {
    "_cell_guid": "bcddf034-4a10-2026-55ea-dd3d969a297b"
   },
   "outputs": [],
   "source": [
    "advancedmodel = RandomForestClassifier()\n",
    "advancedmodel = advancedmodel.fit(advancedtrain, train[\"Label\"])\n",
    "testheadlines = []\n",
    "for row in range(0,len(test.index)):\n",
    "    testheadlines.append(' '.join(str(x) for x in test.iloc[row,1:2]))\n",
    "advancedtest = advancedvectorizer.transform(testheadlines)\n",
    "preds7 = advancedmodel.predict(advancedtest)\n",
    "acc7 = accuracy_score(test['Label'], preds7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {
    "_cell_guid": "e395e3a6-7bc8-a434-16af-be13aebbabbd"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "RF 2 accuracy:  0.3688212927756654\n"
     ]
    }
   ],
   "source": [
    "print('RF 2 accuracy: ', acc7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "0a12329a-12c7-aa90-a138-025087dd76ba"
   },
   "source": [
    "## Gradient Boosting Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ea1681ab-166f-20a6-3c29-f34f84438a36"
   },
   "source": [
    "### GBM 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {
    "_cell_guid": "4ec11f0e-6c50-8149-93c3-acd17d1e7caa"
   },
   "outputs": [],
   "source": [
    "advancedvectorizer = TfidfVectorizer( min_df=0.1, max_df=0.9, max_features = 200000, ngram_range = (1, 1))\n",
    "advancedtrain = advancedvectorizer.fit_transform(trainheadlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {
    "_cell_guid": "3c5d312d-555f-f353-606e-2f560631033a"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(2195, 250)\n"
     ]
    }
   ],
   "source": [
    "print(advancedtrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {
    "_cell_guid": "8f9d72ff-fb8e-5d1a-6016-3ebdb76a2ffd"
   },
   "outputs": [],
   "source": [
    "advancedmodel = GradientBoostingClassifier()\n",
    "advancedmodel = advancedmodel.fit(advancedtrain, train[\"Label\"])\n",
    "testheadlines = []\n",
    "for row in range(0,len(test.index)):\n",
    "    testheadlines.append(' '.join(str(x) for x in test.iloc[row,1:2]))\n",
    "advancedtest = advancedvectorizer.transform(testheadlines)\n",
    "preds8 = advancedmodel.predict(advancedtest.toarray())\n",
    "acc8 = accuracy_score(test['Label'], preds8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {
    "_cell_guid": "ba8ae675-7e71-e24e-69bf-99d5d19590d8"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "GBM 1 accuracy:  0.3840304182509506\n"
     ]
    }
   ],
   "source": [
    "print('GBM 1 accuracy: ', acc8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e39653db-00b9-d1e0-d82e-87105e0eba1e"
   },
   "source": [
    "### GBM 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "_cell_guid": "d35d6a94-69f4-5370-6d0f-5b98c91d2565"
   },
   "outputs": [],
   "source": [
    "advancedvectorizer = TfidfVectorizer( min_df=0.02, max_df=0.175, max_features = 200000, ngram_range = (2, 2))\n",
    "advancedtrain = advancedvectorizer.fit_transform(trainheadlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {
    "_cell_guid": "b83bc122-5059-665a-28c1-85145787155e"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(2195, 765)\n"
     ]
    }
   ],
   "source": [
    "print(advancedtrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {
    "_cell_guid": "e98ca989-15e6-b785-f282-4e59f849cd1f"
   },
   "outputs": [],
   "source": [
    "advancedmodel = GradientBoostingClassifier()\n",
    "advancedmodel = advancedmodel.fit(advancedtrain, train[\"Label\"])\n",
    "testheadlines = []\n",
    "for row in range(0,len(test.index)):\n",
    "    testheadlines.append(' '.join(str(x) for x in test.iloc[row,1:2]))\n",
    "advancedtest = advancedvectorizer.transform(testheadlines)\n",
    "preds9 = advancedmodel.predict(advancedtest.toarray())\n",
    "acc9 = accuracy_score(test['Label'], preds9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {
    "_cell_guid": "6df5f34f-a315-c1c4-fdbe-c54127468d5e"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "GBM 2 accuracy:  0.3688212927756654\n"
     ]
    }
   ],
   "source": [
    "print('GBM 2 accuracy: ', acc9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b7bd5500-e077-8225-31b5-a9c3f1015ca5"
   },
   "source": [
    "## Stochastic Gradient Descent Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8f139d81-25c5-ba11-8924-e68b6e2e55f6"
   },
   "source": [
    "### SGDClassifier 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {
    "_cell_guid": "d071bb2f-df6a-1657-1af8-5d31ee8e4a19"
   },
   "outputs": [],
   "source": [
    "advancedvectorizer = TfidfVectorizer( min_df=0.2, max_df=0.8, max_features = 200000, ngram_range = (1, 1))\n",
    "advancedtrain = advancedvectorizer.fit_transform(trainheadlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {
    "_cell_guid": "24d0156a-2705-fece-001e-0426da57d383"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(2195, 108)\n"
     ]
    }
   ],
   "source": [
    "print(advancedtrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {
    "_cell_guid": "30e4b846-af7e-1eda-dade-f1550d6e043b"
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'n_iter'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-390-d17a39ba8c3e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0madvancedmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSGDClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'modified_huber'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0madvancedmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madvancedmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madvancedtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Label\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtestheadlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mtestheadlines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'n_iter'"
     ]
    }
   ],
   "source": [
    "advancedmodel = SGDClassifier(loss='modified_huber', n_iter=5, random_state=0, shuffle=True)\n",
    "advancedmodel = advancedmodel.fit(advancedtrain, train[\"Label\"])\n",
    "testheadlines = []\n",
    "for row in range(0,len(test.index)):\n",
    "    testheadlines.append(' '.join(str(x) for x in test.iloc[row,1:2]))\n",
    "advancedtest = advancedvectorizer.transform(testheadlines)\n",
    "preds10 = advancedmodel.predict(advancedtest.toarray())\n",
    "acc10 = accuracy_score(test['Label'], preds10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "_cell_guid": "79a91d72-1cd5-4308-9b0e-9810314cdd1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier 1:  0.505291005291\n"
     ]
    }
   ],
   "source": [
    "print('SGDClassifier 1: ', acc10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "0f46cdfe-20b2-9bc6-103a-1f44e2172789"
   },
   "source": [
    "### SGDClassifier 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "_cell_guid": "8ccd7ace-7326-4dfe-b93d-a8377e1a3ba6"
   },
   "outputs": [],
   "source": [
    "advancedvectorizer = TfidfVectorizer( min_df=0.03, max_df=0.2, max_features = 200000, ngram_range = (2, 2))\n",
    "advancedtrain = advancedvectorizer.fit_transform(trainheadlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "_cell_guid": "c012b95d-8575-ef34-d699-d240ade631f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1611, 631)\n"
     ]
    }
   ],
   "source": [
    "print(advancedtrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "_cell_guid": "5e4f2994-e199-976d-f54b-0e676b6b11be"
   },
   "outputs": [],
   "source": [
    "advancedmodel = SGDClassifier(loss='modified_huber', n_iter=5, random_state=0, shuffle=True)\n",
    "advancedmodel = advancedmodel.fit(advancedtrain, train[\"Label\"])\n",
    "testheadlines = []\n",
    "for row in range(0,len(test.index)):\n",
    "    testheadlines.append(' '.join(str(x) for x in test.iloc[row,1:2]))\n",
    "advancedtest = advancedvectorizer.transform(testheadlines)\n",
    "preds11 = advancedmodel.predict(advancedtest.toarray())\n",
    "acc11 = accuracy_score(test['Label'], preds11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "_cell_guid": "a9d8499e-ae6a-6197-c86f-c31bf73f3f98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier 2:  0.542328042328\n"
     ]
    }
   ],
   "source": [
    "print('SGDClassifier 2: ', acc11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d6aa0178-e7f3-6427-f88f-aa6e436fc87d"
   },
   "source": [
    "## Naive Bayes SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "_cell_guid": "cd1391ca-1b70-13c0-3a62-151fc84f6069"
   },
   "outputs": [],
   "source": [
    "class NBSVM(six.with_metaclass(ABCMeta, BaseEstimator, ClassifierMixin)):\n",
    "\n",
    "    def __init__(self, alpha=1.0, C=1.0, max_iter=10000):\n",
    "        self.alpha = alpha\n",
    "        self.max_iter = max_iter\n",
    "        self.C = C\n",
    "        self.svm_ = [] # fuggly\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X, y = check_X_y(X, y, 'csr')\n",
    "        _, n_features = X.shape\n",
    "\n",
    "        labelbin = LabelBinarizer()\n",
    "        Y = labelbin.fit_transform(y)\n",
    "        self.classes_ = labelbin.classes_\n",
    "        if Y.shape[1] == 1:\n",
    "            Y = np.concatenate((1 - Y, Y), axis=1)\n",
    "\n",
    "        # LabelBinarizer().fit_transform() returns arrays with dtype=np.int64.\n",
    "        # so we don't have to cast X to floating point\n",
    "        Y = Y.astype(np.float64)\n",
    "\n",
    "        # Count raw events from data\n",
    "        n_effective_classes = Y.shape[1]\n",
    "        self.class_count_ = np.zeros(n_effective_classes, dtype=np.float64)\n",
    "        self.ratios_ = np.full((n_effective_classes, n_features), self.alpha,\n",
    "                                 dtype=np.float64)\n",
    "        self._compute_ratios(X, Y)\n",
    "\n",
    "        # flugglyness\n",
    "        for i in range(n_effective_classes):\n",
    "            X_i = X.multiply(self.ratios_[i])\n",
    "            svm = LinearSVC(C=self.C, max_iter=self.max_iter)\n",
    "            Y_i = Y[:,i]\n",
    "            svm.fit(X_i, Y_i)\n",
    "            self.svm_.append(svm) \n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        n_effective_classes = self.class_count_.shape[0]\n",
    "        n_examples = X.shape[0]\n",
    "\n",
    "        D = np.zeros((n_effective_classes, n_examples))\n",
    "\n",
    "        for i in range(n_effective_classes):\n",
    "            X_i = X.multiply(self.ratios_[i])\n",
    "            D[i] = self.svm_[i].decision_function(X_i)\n",
    "        \n",
    "        return self.classes_[np.argmax(D, axis=0)]\n",
    "        \n",
    "    def _compute_ratios(self, X, Y):\n",
    "        \"\"\"Count feature occurrences and compute ratios.\"\"\"\n",
    "        if np.any((X.data if issparse(X) else X) < 0):\n",
    "            raise ValueError(\"Input X must be non-negative\")\n",
    "\n",
    "        self.ratios_ += safe_sparse_dot(Y.T, X)  # ratio + feature_occurrance_c\n",
    "        normalize(self.ratios_, norm='l1', axis=1, copy=False)\n",
    "        row_calc = lambda r: np.log(np.divide(r, (1 - r)))\n",
    "        self.ratios_ = np.apply_along_axis(row_calc, axis=1, arr=self.ratios_)\n",
    "        check_array(self.ratios_)\n",
    "        self.ratios_ = sparse.csr_matrix(self.ratios_)\n",
    "\n",
    "        #p_c /= np.linalg.norm(p_c, ord=1)\n",
    "        #ratios[c] = np.log(p_c / (1 - p_c))\n",
    "\n",
    "\n",
    "def f1_class(pred, truth, class_val):\n",
    "    n = len(truth)\n",
    "\n",
    "    truth_class = 0\n",
    "    pred_class = 0\n",
    "    tp = 0\n",
    "\n",
    "    for ii in range(0, n):\n",
    "        if truth[ii] == class_val:\n",
    "            truth_class += 1\n",
    "            if truth[ii] == pred[ii]:\n",
    "                tp += 1\n",
    "                pred_class += 1\n",
    "                continue;\n",
    "        if pred[ii] == class_val:\n",
    "            pred_class += 1\n",
    "\n",
    "    precision = tp / float(pred_class)\n",
    "    recall = tp / float(truth_class)\n",
    "\n",
    "    return (2.0 * precision * recall) / (precision + recall)\n",
    "\n",
    "\n",
    "def semeval_senti_f1(pred, truth, pos=2, neg=0): \n",
    "\n",
    "    f1_pos = f1_class(pred, truth, pos)\n",
    "    f1_neg = f1_class(pred, truth, neg)\n",
    "\n",
    "    return (f1_pos + f1_neg) / 2.0;\n",
    "\n",
    "\n",
    "def main(train_file, test_file, ngram=(1, 3)):\n",
    "    print('loading...')\n",
    "    train = pd.read_csv(train_file, delimiter='\\t', encoding='utf-8', header=0,\n",
    "                        names=['text', 'label'])\n",
    "\n",
    "    # to shuffle:\n",
    "    #train.iloc[np.random.permutation(len(df))]\n",
    "\n",
    "    test = pd.read_csv(test_file, delimiter='\\t', encoding='utf-8', header=0,\n",
    "                        names=['text', 'label'])\n",
    "\n",
    "    print('vectorizing...')\n",
    "    vect = CountVectorizer()\n",
    "    classifier = NBSVM()\n",
    "\n",
    "    # create pipeline\n",
    "    clf = Pipeline([('vect', vect), ('nbsvm', classifier)])\n",
    "    params = {\n",
    "        'vect__token_pattern': r\"\\S+\",\n",
    "        'vect__ngram_range': ngram, \n",
    "        'vect__binary': True\n",
    "    }\n",
    "    clf.set_params(**params)\n",
    "\n",
    "    #X_train = vect.fit_transform(train['text'])\n",
    "    #X_test = vect.transform(test['text'])\n",
    "\n",
    "    print('fitting...')\n",
    "    clf.fit(train['text'], train['label'])\n",
    "\n",
    "    print('classifying...')\n",
    "    pred = clf.predict(test['text'])\n",
    "   \n",
    "    print('testing...')\n",
    "    acc = accuracy_score(test['label'], pred)\n",
    "    f1 = semeval_senti_f1(pred, test['label'])\n",
    "    print('NBSVM: acc=%f, f1=%f' % (acc, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7f0e000a-ca1a-5611-0ca8-c8bc40eafecb"
   },
   "source": [
    "### NBSVM 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "_cell_guid": "e054e5ea-c8be-cb98-90a1-2512b3478b6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1611, 535)\n"
     ]
    }
   ],
   "source": [
    "advancedvectorizer = TfidfVectorizer( min_df=0.1, max_df=0.8, max_features = 200000, ngram_range = (1, 1))\n",
    "advancedtrain = advancedvectorizer.fit_transform(trainheadlines)\n",
    "print(advancedtrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "_cell_guid": "b57a2b30-d69f-cf0a-05d0-8a065d5a758b"
   },
   "outputs": [],
   "source": [
    "advancedmodel = NBSVM(C=0.01)\n",
    "advancedmodel = advancedmodel.fit(advancedtrain, train[\"Label\"])\n",
    "testheadlines = []\n",
    "for row in range(0,len(test.index)):\n",
    "    testheadlines.append(' '.join(str(x) for x in test.iloc[row,1:2]))\n",
    "advancedtest = advancedvectorizer.transform(testheadlines)\n",
    "preds12 = advancedmodel.predict(advancedtest)\n",
    "acc12 = accuracy_score(test['Label'], preds12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "_cell_guid": "6acdade4-665e-5c06-95af-3e5ddcfd33d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NBSVM 1:  0.478835978836\n"
     ]
    }
   ],
   "source": [
    "print('NBSVM 1: ', acc12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "aca410f2-118d-f973-6b14-a63048444f26"
   },
   "source": [
    "### NBSVM 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "_cell_guid": "663e5e98-7ccd-29a4-40a8-10a46327c06b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1611, 613)\n"
     ]
    }
   ],
   "source": [
    "advancedvectorizer = TfidfVectorizer( min_df=0.031, max_df=0.2, max_features = 200000, ngram_range = (2, 2))\n",
    "advancedtrain = advancedvectorizer.fit_transform(trainheadlines)\n",
    "print(advancedtrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "_cell_guid": "0c67fed4-8110-2a52-bb33-1da6f294b592"
   },
   "outputs": [],
   "source": [
    "advancedmodel = NBSVM(C=0.01)\n",
    "advancedmodel = advancedmodel.fit(advancedtrain, train[\"Label\"])\n",
    "testheadlines = []\n",
    "for row in range(0,len(test.index)):\n",
    "    testheadlines.append(' '.join(str(x) for x in test.iloc[row,1:2]))\n",
    "advancedtest = advancedvectorizer.transform(testheadlines)\n",
    "preds13 = advancedmodel.predict(advancedtest)\n",
    "acc13 = accuracy_score(test['Label'], preds13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "_cell_guid": "803861e3-d740-c2ed-2b2e-57221ba9b8c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NBSVM 2:  0.595238095238\n"
     ]
    }
   ],
   "source": [
    "print('NBSVM 2: ', acc13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "682e897f-169c-8694-d05d-66e9fc844158"
   },
   "source": [
    "## Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1de5a061-52d5-2578-9cb0-2602499ad87b"
   },
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "_cell_guid": "ac32628f-8080-5d53-e3a2-3d6986431f15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1611, 401)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "nb_classes = 2\n",
    "advancedvectorizer = TfidfVectorizer( min_df=0.04, max_df=0.3, max_features = 200000, ngram_range = (2, 2))\n",
    "advancedtrain = advancedvectorizer.fit_transform(trainheadlines)\n",
    "testheadlines = []\n",
    "for row in range(0,len(test.index)):\n",
    "    testheadlines.append(' '.join(str(x) for x in test.iloc[row,1:2]))\n",
    "advancedtest = advancedvectorizer.transform(testheadlines)\n",
    "print(advancedtrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "_cell_guid": "eca75f7e-4fe5-0be4-85ee-47fecf3ad4d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (1611, 401)\n",
      "X_test shape: (378, 401)\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/Keras-1.2.2-py3.6.egg/keras/models.py:654: UserWarning: The \"show_accuracy\" argument is deprecated, instead you should pass the \"accuracy\" metric to the model at compile time:\n",
      "`model.compile(optimizer, loss, metrics=[\"accuracy\"])`\n",
      "  warnings.warn('The \"show_accuracy\" argument is deprecated, '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1369 samples, validate on 242 samples\n",
      "Epoch 1/2\n",
      "1369/1369 [==============================] - 0s - loss: 0.6953 - val_loss: 0.6879\n",
      "Epoch 2/2\n",
      "1369/1369 [==============================] - 0s - loss: 0.6685 - val_loss: 0.6850\n",
      "Generating test predictions...\n"
     ]
    }
   ],
   "source": [
    "X_train = advancedtrain.toarray()\n",
    "X_test = advancedtest.toarray()\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "y_train = np.array(train[\"Label\"])\n",
    "y_test = np.array(test[\"Label\"])\n",
    "\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "\n",
    "# pre-processing: divide by max and substract mean\n",
    "scale = np.max(X_train)\n",
    "X_train /= scale\n",
    "X_test /= scale\n",
    "\n",
    "mean = np.mean(X_train)\n",
    "X_train -= mean\n",
    "X_test -= mean\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "\n",
    "# Here's a Deep Dumb MLP (DDMLP)\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=input_dim))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# we'll use categorical xent for the loss, and RMSprop as the optimizer\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "print(\"Training...\")\n",
    "model.fit(X_train, Y_train, nb_epoch=2, batch_size=16, validation_split=0.15, show_accuracy=True)\n",
    "\n",
    "print(\"Generating test predictions...\")\n",
    "preds14 = model.predict_classes(X_test, verbose=0)\n",
    "acc14 = accuracy_score(test[\"Label\"], preds14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "_cell_guid": "39345ebd-2cf2-8332-6fb2-d00bc801af57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction accuracy:  0.547619047619\n"
     ]
    }
   ],
   "source": [
    "print('prediction accuracy: ', acc14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "143c5d6d-4259-c945-f1dd-b07a4c388526"
   },
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "_cell_guid": "db232b70-f3a0-0952-e7ab-9c0b768e8025"
   },
   "outputs": [],
   "source": [
    "max_features = 10000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.1\n",
    "maxlen = 200\n",
    "batch_size = 32\n",
    "nb_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "_cell_guid": "e5f6e70a-325d-5a94-0c16-c852f951d475"
   },
   "outputs": [],
   "source": [
    "# vectorize the text samples into a 2D integer tensor\n",
    "tokenizer = Tokenizer(nb_words=max_features)\n",
    "tokenizer.fit_on_texts(trainheadlines)\n",
    "sequences_train = tokenizer.texts_to_sequences(trainheadlines)\n",
    "sequences_test = tokenizer.texts_to_sequences(testheadlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "_cell_guid": "a9224d6a-3bca-ef84-9817-a72b41241f54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n",
      "X_train shape: (1611, 200)\n",
      "X_test shape: (378, 200)\n"
     ]
    }
   ],
   "source": [
    "print('Pad sequences (samples x time)')\n",
    "X_train = sequence.pad_sequences(sequences_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(sequences_test, maxlen=maxlen)\n",
    "\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "_cell_guid": "29359f0d-8d42-2038-c04f-2cd5bc87bc93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "Train...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:91: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1611 samples, validate on 378 samples\n",
      "Epoch 1/3\n",
      "1611/1611 [==============================] - 50s - loss: 0.6942 - acc: 0.5208 - val_loss: 0.6935 - val_acc: 0.5079\n",
      "Epoch 2/3\n",
      "1611/1611 [==============================] - 51s - loss: 0.6722 - acc: 0.5971 - val_loss: 0.6904 - val_acc: 0.5106\n",
      "Epoch 3/3\n",
      "1611/1611 [==============================] - 50s - loss: 0.5941 - acc: 0.7492 - val_loss: 0.6831 - val_acc: 0.5661\n",
      "378/378 [==============================] - 3s     \n",
      "Test score: 0.683092702633\n",
      "Test accuracy: 0.566137566138\n",
      "Generating test predictions...\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128, dropout=0.2))\n",
    "model.add(LSTM(128, dropout_W=0.2, dropout_U=0.2)) \n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=3,\n",
    "          validation_data=(X_test, Y_test))\n",
    "score, acc = model.evaluate(X_test, Y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)\n",
    "\n",
    "\n",
    "print(\"Generating test predictions...\")\n",
    "preds15 = model.predict_classes(X_test, verbose=0)\n",
    "acc15 = accuracy_score(test['Label'], preds15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "_cell_guid": "365d4d4b-57cd-c678-6e62-133aa32586fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction accuracy:  0.566137566138\n"
     ]
    }
   ],
   "source": [
    "print('prediction accuracy: ', acc15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8afb0d07-7d0d-dd18-b19c-be603f301b52"
   },
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "_cell_guid": "7271a1d1-cde0-a26c-de99-edfb7dfbf918"
   },
   "outputs": [],
   "source": [
    "nb_filter = 120\n",
    "filter_length = 2\n",
    "hidden_dims = 120\n",
    "nb_epoch = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "_cell_guid": "c071e2fc-2106-4bc5-2e2f-dde4a84dabdf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128, dropout=0.2))\n",
    "# we add a Convolution1D, which will learn nb_filter\n",
    "# word group filters of size filter_length:\n",
    "model.add(Convolution1D(nb_filter=nb_filter,\n",
    "                        filter_length=filter_length,\n",
    "                        border_mode='valid',\n",
    "                        activation='relu',\n",
    "                        subsample_length=1))\n",
    "\n",
    "def max_1d(X):\n",
    "    return K.max(X, axis=1)\n",
    "\n",
    "model.add(Lambda(max_1d, output_shape=(nb_filter,)))\n",
    "model.add(Dense(hidden_dims)) \n",
    "model.add(Dropout(0.2)) \n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "_cell_guid": "61e07990-43ef-ff27-3b36-dca146f8b4a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:91: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1611 samples, validate on 378 samples\n",
      "Epoch 1/1\n",
      "1611/1611 [==============================] - 14s - loss: 0.6900 - acc: 0.5317 - val_loss: 0.6974 - val_acc: 0.5079\n",
      "352/378 [==========================>...] - ETA: 0sTest score: 0.69742725829\n",
      "Test accuracy: 0.507936509829\n",
      "Generating test predictions...\n"
     ]
    }
   ],
   "source": [
    "print('Train...')\n",
    "model.fit(X_train, Y_train, batch_size=32, nb_epoch=1,\n",
    "          validation_data=(X_test, Y_test))\n",
    "score, acc = model.evaluate(X_test, Y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)\n",
    "\n",
    "\n",
    "print(\"Generating test predictions...\")\n",
    "preds16 = model.predict_classes(X_test, verbose=0)\n",
    "acc16 = accuracy_score(test['Label'], preds16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "_cell_guid": "49e9abbe-d224-b757-c17b-bd4cfa5da818"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction accuracy:  0.507936507937\n"
     ]
    }
   ],
   "source": [
    "print('prediction accuracy: ', acc16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "_cell_guid": "21b86295-1a3b-a1b8-78cf-b032b9d74eaa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "_cell_guid": "91aa05fb-b8fe-fe99-d191-c08b8dc5620c"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "_cell_guid": "60fa6ce4-1658-11d2-c0ad-3b43a542fff0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "_cell_guid": "03273578-b025-4e87-6088-a993fce31e2f"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "_change_revision": 1261,
  "_is_fork": false,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "e65a3f05640f455e06a18a2560213b8a569821117be7329f041e0ef559264675"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}