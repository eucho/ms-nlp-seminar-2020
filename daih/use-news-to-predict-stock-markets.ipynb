{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "63de4277-c079-1630-2e47-44d138f3e7bf"
   },
   "source": [
    "# Library Import #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_cell_guid": "193a91a4-7404-a01b-2c78-6b7385979b9c"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "english_stemmer=nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "from sklearn.feature_selection.univariate_selection import SelectKBest, chi2, f_classif\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.linear_model import SGDClassifier, SGDRegressor,LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "import sys\n",
    "import os, glob\n",
    "import argparse\n",
    "from sklearn.pipeline import Pipeline\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import six\n",
    "from abc import ABCMeta\n",
    "from scipy import sparse\n",
    "from scipy.sparse import issparse\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils import check_X_y, check_array\n",
    "from sklearn.utils.extmath import safe_sparse_dot\n",
    "from sklearn.preprocessing import normalize, binarize, LabelBinarizer\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Lambda\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM, SimpleRNN, GRU\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from collections import defaultdict\n",
    "from keras.layers.convolutional import Convolution1D\n",
    "from keras import backend as K\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from simpletransformers.classification import ClassificationModel, ClassificationArgs\n",
    "import logging\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f98318d0-c122-24a1-a9a6-62ab93985428"
   },
   "source": [
    "# Data Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c39b53ac-2080-bfb4-124d-5e775af2cec2"
   },
   "source": [
    "we'll use all of the dates up to the end of 2014 as our training data and everything after as testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "_cell_guid": "28217d5a-e580-fa92-fdc2-45a4842812e0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-97-b541aefa82bb>:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  binaryLabel[\"Date\"] = pd.to_datetime(binaryLabel[\"Date\"])\n",
      "<ipython-input-97-b541aefa82bb>:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  binaryLabel['Date']=binaryLabel['Date'].dt.strftime('%Y-%m-%d')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Top26</th>\n",
       "      <th>Top1</th>\n",
       "      <th>Top2</th>\n",
       "      <th>Top3</th>\n",
       "      <th>Top4</th>\n",
       "      <th>Top5</th>\n",
       "      <th>Top6</th>\n",
       "      <th>Top7</th>\n",
       "      <th>Top8</th>\n",
       "      <th>...</th>\n",
       "      <th>Top19</th>\n",
       "      <th>Top20</th>\n",
       "      <th>Top21</th>\n",
       "      <th>Top22</th>\n",
       "      <th>Top23</th>\n",
       "      <th>Top24</th>\n",
       "      <th>Top25</th>\n",
       "      <th>Label</th>\n",
       "      <th>Label_4part</th>\n",
       "      <th>Label_3part</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-05-04</td>\n",
       "      <td>Be sure to tune in and watch Donald Trump on L...</td>\n",
       "      <td>b'Saudi Arabia to consider banning marriage of...</td>\n",
       "      <td>b'Pregnant woman miscarries after traffic acci...</td>\n",
       "      <td>b'In four past pandemics, a mild spring outbre...</td>\n",
       "      <td>b'20 year old pregnant woman faces mandatory d...</td>\n",
       "      <td>b\"Without a Moral Leg to Stand On; United Arab...</td>\n",
       "      <td>b\"Meet Noemi Letizia, the 18 year old hottie b...</td>\n",
       "      <td>b'So while everyone was fearing the Swine Flu ...</td>\n",
       "      <td>b'Defense Minister Ehud Barak said he would ca...</td>\n",
       "      <td>...</td>\n",
       "      <td>b'An interview with a jailed Somali pirate lea...</td>\n",
       "      <td>b'Delara Darabi: \"Oh mother, I see the hangman...</td>\n",
       "      <td>b'SARS sleuth attacks WHO for being worthless ...</td>\n",
       "      <td>b'Targeted by Taliban, Sikhs flee Pak region; ...</td>\n",
       "      <td>b'Israel Faces a Hard Sell: \"The American, Eur...</td>\n",
       "      <td>b'French Navy said they seized 11 pirates Sund...</td>\n",
       "      <td>b'More Gaza patients quizzed at crossings, pre...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2009-05-08</td>\n",
       "      <td>Donald Trump reads Top Ten Financial Tips on L...</td>\n",
       "      <td>b'Eric Arthur Blair  aka George Orwell must be...</td>\n",
       "      <td>b'British Muslim Immigrants More Strongly Iden...</td>\n",
       "      <td>b\"Irish student's fake quote on the Wikipedia ...</td>\n",
       "      <td>b\"EU wants 'Internet G12' to govern cyberspace...</td>\n",
       "      <td>b\"India's Election: how democracy may turn the...</td>\n",
       "      <td>b'Dr. Kamal Labwani, a prominent political and...</td>\n",
       "      <td>b'\"We want to leave the city, but we cannot go...</td>\n",
       "      <td>b'Afghanistan: NATO Should Come Clean on White...</td>\n",
       "      <td>...</td>\n",
       "      <td>b\"1 million people now displaced by fighting i...</td>\n",
       "      <td>b'Sikh police in Britain push for bulletproof ...</td>\n",
       "      <td>b'Gunmen kill Darfur peacekeeper during carjac...</td>\n",
       "      <td>b'Saudi beauty queen: Veiled face and judged o...</td>\n",
       "      <td>b'Sierra Leone child miners: Legacy of conflict '</td>\n",
       "      <td>b'CODEPINK Mothers Day 2009: \"I will not raise...</td>\n",
       "      <td>b'Gunman shoots three police, holed up in hous...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2009-05-12</td>\n",
       "      <td>\"My persona will never be that of a wallflower...</td>\n",
       "      <td>b'The PirateBay launches a D-DO$ attack. Wonde...</td>\n",
       "      <td>b'Israeli academics on the problems with Wikip...</td>\n",
       "      <td>b\"Change:  China overtakes the US as Brazil's ...</td>\n",
       "      <td>b'Legalising Drugs: Lessons From Portugal'</td>\n",
       "      <td>b'Video of a Guatemalan lawyer filmed the day ...</td>\n",
       "      <td>b'The World Health Organization is investigati...</td>\n",
       "      <td>b'Cocaine Purity down, Prices up'</td>\n",
       "      <td>b'Guatemala: In YouTube Video Shot Before His ...</td>\n",
       "      <td>...</td>\n",
       "      <td>b'Sri Lankan government forces shells hospital...</td>\n",
       "      <td>b\"The man Netanyahu plans to make Israel's new...</td>\n",
       "      <td>b'Doctor says 49 killed in Sri Lanka hospital ...</td>\n",
       "      <td>b'49 killed in Sri Lanka hospital attack'</td>\n",
       "      <td>b\"This is London  the capital of Somali pirate...</td>\n",
       "      <td>b'Russia holds Victory Day celebration: rows o...</td>\n",
       "      <td>b'Torture: It Was Never Just Sleep Deprivation'</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2009-05-13</td>\n",
       "      <td>Listen to an interview with Donald Trump discu...</td>\n",
       "      <td>b'Swedish Bank to Freeze Accounts of The Pirat...</td>\n",
       "      <td>b'Guess who back in the TED house? Hans Roslin...</td>\n",
       "      <td>b'UK ISPs refuse to play Internet copyright cops'</td>\n",
       "      <td>b'Pope demands independent Palestinian state: ...</td>\n",
       "      <td>b'Five men beheaded in Saudi Arabia after gros...</td>\n",
       "      <td>b'The Vatican made a surprise denial today tha...</td>\n",
       "      <td>b\"Sri Lanka has written off thousands of its o...</td>\n",
       "      <td>b'Guatemalan president accused of murder in vi...</td>\n",
       "      <td>...</td>\n",
       "      <td>b\"Roxana Saberi, the journalist recently relea...</td>\n",
       "      <td>b'Pope tells Palestinians: \"the wall that intr...</td>\n",
       "      <td>b'Kremlin: Battles over energy may lead to war...</td>\n",
       "      <td>b'MDC activist dies from ZANU-PF militia tortu...</td>\n",
       "      <td>b'Somalia:  Islamist fighters poised to topple...</td>\n",
       "      <td>b'Data-Mining Predicted Swine Flu 18 Days befo...</td>\n",
       "      <td>b\"Scientists in Bolivia say that one of the co...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009-05-14</td>\n",
       "      <td>\"Strive for wholeness and keep your sense of w...</td>\n",
       "      <td>b'\"I was murdered by president Alvaro Colom\"'</td>\n",
       "      <td>b'\"Everyone is very angry with this wretched A...</td>\n",
       "      <td>b'Israel moves on plan to annex Palestinian la...</td>\n",
       "      <td>b\"Russia warns of war within a decade over Arc...</td>\n",
       "      <td>b'40,000-year-old figurine thought to be the o...</td>\n",
       "      <td>b'Gaza man: Israeli troops forced me to drink ...</td>\n",
       "      <td>b'Russia warns of war over Arctic resources'</td>\n",
       "      <td>b\"Gordon Brown's government has repeatedly bla...</td>\n",
       "      <td>...</td>\n",
       "      <td>b'British Association for Adoption and Fosteri...</td>\n",
       "      <td>b'Finland ignored warnings of prisoner prostit...</td>\n",
       "      <td>b'Men should hold onto dongs... [Seriously, th...</td>\n",
       "      <td>b'Obsession with Naked Women Dates Back 35,000...</td>\n",
       "      <td>b\"Burma's bad ass ruling military junta: They ...</td>\n",
       "      <td>b\"Saudi Judge Causes Uproar For Saying It's Ok...</td>\n",
       "      <td>b'Swine flu infected Mexico\\'s relations with ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2009-05-15</td>\n",
       "      <td>Enter the \"Think Like A Champion\" signed book ...</td>\n",
       "      <td>b' Unsung hero : The only reason we know anyth...</td>\n",
       "      <td>b\"Noticed an unusual, strong burst of pro-isra...</td>\n",
       "      <td>b'The world elite meets again for Bilderberg: ...</td>\n",
       "      <td>b' Boycott Israel campaign starts to bite'</td>\n",
       "      <td>b'\"If you are watching this message, it is bec...</td>\n",
       "      <td>b'Muslim famous for predicting a totally Islam...</td>\n",
       "      <td>b'Australian immigration authorities told a de...</td>\n",
       "      <td>b'Guatemala arrests Twitter user for inciting ...</td>\n",
       "      <td>...</td>\n",
       "      <td>b\"Shadowy Bilderberg group meet in Greece--and...</td>\n",
       "      <td>b'Vietnam veteran on spiritual quest lands Aun...</td>\n",
       "      <td>b'Avigdor Lieberman - To ban Israeli Arabs fro...</td>\n",
       "      <td>b'Paul Craig Roberts on the Global Anti-Semiti...</td>\n",
       "      <td>b'The rich, shadowy Bilderberg group'</td>\n",
       "      <td>b\"The World's Best Illusion: The Secret of the...</td>\n",
       "      <td>b\"Defining 'Success' Down\"</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2009-05-18</td>\n",
       "      <td>\"We win in our lives by having a champion's vi...</td>\n",
       "      <td>b'The leader of the Tamil Tiger rebels, Velupi...</td>\n",
       "      <td>b'Former Mexican President Vicente Fox urges t...</td>\n",
       "      <td>b\"Water is becoming privatized and may soon be...</td>\n",
       "      <td>b'Canadian detectives beer knowledge leads to ...</td>\n",
       "      <td>b'I give it 72 hours tops before 4chan has hac...</td>\n",
       "      <td>b'Banks like Citigroup-owned Banamex get away ...</td>\n",
       "      <td>b\" Sabotage hits Britain's biggest cycle race\"</td>\n",
       "      <td>b'Leading Muslim Scholars in Pakistan Denounce...</td>\n",
       "      <td>...</td>\n",
       "      <td>b'Australian topless bathing ban urged'</td>\n",
       "      <td>b'Guatemala: Thousands protest over allegation...</td>\n",
       "      <td>b'James Lull, Ponzi Scammer, Drives Truck Off ...</td>\n",
       "      <td>b'Darfur rebel leader surrenders to war crimes...</td>\n",
       "      <td>b'Woman, 66, defends right to have baby!'</td>\n",
       "      <td>b'Indian markets surge on Congress win'</td>\n",
       "      <td>b'The hijacking of merchant ships by Somali pi...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2009-05-19</td>\n",
       "      <td>\"...these days...we could all use a little of ...</td>\n",
       "      <td>b'The Belgian bodybuilding championship has be...</td>\n",
       "      <td>b\"Charlie Brooker: The BNP represents Britain'...</td>\n",
       "      <td>b'Excessive cola consumption can lead to anyth...</td>\n",
       "      <td>b\"'Ghost airports' - South Korea's new airport...</td>\n",
       "      <td>b'At least 153 of the newly-elected MPs in Ind...</td>\n",
       "      <td>b'Gordon Brown announces any MP who \"defied th...</td>\n",
       "      <td>b'Sri Lanka: Chronology of the key dates in th...</td>\n",
       "      <td>b'Media coverage comparison of the conflicts i...</td>\n",
       "      <td>...</td>\n",
       "      <td>b'Leader of the Tamil Tigers killed. It really...</td>\n",
       "      <td>b'Ethiopian military forces have crossed back ...</td>\n",
       "      <td>b'Careful, sober analysis of situation in Guat...</td>\n",
       "      <td>b\"Sri Lanka leader hails 'victory'\"</td>\n",
       "      <td>b'Young Pakistanis Take One Problem Into Their...</td>\n",
       "      <td>b'Thailand:  Tourist faces between 2 and 5 yea...</td>\n",
       "      <td>b'MPs on all sides tell the Speaker Michael Ma...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2009-05-20</td>\n",
       "      <td>\"Always know you could be on the precipice of ...</td>\n",
       "      <td>b\"'Endemic' rape and abuse of Irish children i...</td>\n",
       "      <td>b'Catholic Church was aware long-term sex offe...</td>\n",
       "      <td>b'\"The world has ignored our warnings.\" Mohame...</td>\n",
       "      <td>b'Thousands beaten, raped in Irish reform scho...</td>\n",
       "      <td>b'Thousands Rally Against President Colom in G...</td>\n",
       "      <td>b'Netanyahu aide call\\'s Obama\\'s 2-state plan...</td>\n",
       "      <td>b'Quarter of a million Sri Lankans face two ye...</td>\n",
       "      <td>b'Owls and kestrels are being employed as agri...</td>\n",
       "      <td>...</td>\n",
       "      <td>b'Iran test-fired a new advanced missile Wedne...</td>\n",
       "      <td>b'Silvio Berlusconi, \"Basically, we just don\\'...</td>\n",
       "      <td>b'Sri Lanka crisis deepens as Red Cross suspen...</td>\n",
       "      <td>b'Spanish lawmakers move to curb foreign human...</td>\n",
       "      <td>b'Sri Lanka on brink of catastrophe as UN aid ...</td>\n",
       "      <td>b'Myanmar to let diplomats meet with Suu Kyi'</td>\n",
       "      <td>b'Anti-Semitic protestors in Argentina give pr...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2009-05-21</td>\n",
       "      <td>\"Keep it fast, short and direct - whatever it ...</td>\n",
       "      <td>b'The Nigerian military has been accused of ki...</td>\n",
       "      <td>b'Couple do a runner after bank error sees $10...</td>\n",
       "      <td>b'Work for us or we will say you are a terrorist'</td>\n",
       "      <td>b'A 15-year-old girl has been charged with cul...</td>\n",
       "      <td>b\"How fox news' sister channel, sky news, deal...</td>\n",
       "      <td>b'Vancouver 2010 Olympic torch is...uniquely C...</td>\n",
       "      <td>b\"UN Gaza inquiry 'to proceed despite Israel' \"</td>\n",
       "      <td>b' First Italian restaurant opens in Pyongyang...</td>\n",
       "      <td>...</td>\n",
       "      <td>b'The Poor Get Poorer: While Americans are wor...</td>\n",
       "      <td>b'Burma bars international observers from the ...</td>\n",
       "      <td>b'Movie and music industry appeals Pirate Bay ...</td>\n",
       "      <td>b'Lucky couple - Gone missing after bank mista...</td>\n",
       "      <td>b'Immigrants hurt in Greek violence'</td>\n",
       "      <td>b\"US intruder said Suu Kyi 'scared'\"</td>\n",
       "      <td>b'Thousands raped and abused in Catholic schools'</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date                                              Top26  \\\n",
       "0   2009-05-04  Be sure to tune in and watch Donald Trump on L...   \n",
       "1   2009-05-08  Donald Trump reads Top Ten Financial Tips on L...   \n",
       "2   2009-05-12  \"My persona will never be that of a wallflower...   \n",
       "3   2009-05-13  Listen to an interview with Donald Trump discu...   \n",
       "4   2009-05-14  \"Strive for wholeness and keep your sense of w...   \n",
       "5   2009-05-15  Enter the \"Think Like A Champion\" signed book ...   \n",
       "8   2009-05-18  \"We win in our lives by having a champion's vi...   \n",
       "9   2009-05-19  \"...these days...we could all use a little of ...   \n",
       "10  2009-05-20  \"Always know you could be on the precipice of ...   \n",
       "11  2009-05-21  \"Keep it fast, short and direct - whatever it ...   \n",
       "\n",
       "                                                 Top1  \\\n",
       "0   b'Saudi Arabia to consider banning marriage of...   \n",
       "1   b'Eric Arthur Blair  aka George Orwell must be...   \n",
       "2   b'The PirateBay launches a D-DO$ attack. Wonde...   \n",
       "3   b'Swedish Bank to Freeze Accounts of The Pirat...   \n",
       "4       b'\"I was murdered by president Alvaro Colom\"'   \n",
       "5   b' Unsung hero : The only reason we know anyth...   \n",
       "8   b'The leader of the Tamil Tiger rebels, Velupi...   \n",
       "9   b'The Belgian bodybuilding championship has be...   \n",
       "10  b\"'Endemic' rape and abuse of Irish children i...   \n",
       "11  b'The Nigerian military has been accused of ki...   \n",
       "\n",
       "                                                 Top2  \\\n",
       "0   b'Pregnant woman miscarries after traffic acci...   \n",
       "1   b'British Muslim Immigrants More Strongly Iden...   \n",
       "2   b'Israeli academics on the problems with Wikip...   \n",
       "3   b'Guess who back in the TED house? Hans Roslin...   \n",
       "4   b'\"Everyone is very angry with this wretched A...   \n",
       "5   b\"Noticed an unusual, strong burst of pro-isra...   \n",
       "8   b'Former Mexican President Vicente Fox urges t...   \n",
       "9   b\"Charlie Brooker: The BNP represents Britain'...   \n",
       "10  b'Catholic Church was aware long-term sex offe...   \n",
       "11  b'Couple do a runner after bank error sees $10...   \n",
       "\n",
       "                                                 Top3  \\\n",
       "0   b'In four past pandemics, a mild spring outbre...   \n",
       "1   b\"Irish student's fake quote on the Wikipedia ...   \n",
       "2   b\"Change:  China overtakes the US as Brazil's ...   \n",
       "3   b'UK ISPs refuse to play Internet copyright cops'   \n",
       "4   b'Israel moves on plan to annex Palestinian la...   \n",
       "5   b'The world elite meets again for Bilderberg: ...   \n",
       "8   b\"Water is becoming privatized and may soon be...   \n",
       "9   b'Excessive cola consumption can lead to anyth...   \n",
       "10  b'\"The world has ignored our warnings.\" Mohame...   \n",
       "11  b'Work for us or we will say you are a terrorist'   \n",
       "\n",
       "                                                 Top4  \\\n",
       "0   b'20 year old pregnant woman faces mandatory d...   \n",
       "1   b\"EU wants 'Internet G12' to govern cyberspace...   \n",
       "2          b'Legalising Drugs: Lessons From Portugal'   \n",
       "3   b'Pope demands independent Palestinian state: ...   \n",
       "4   b\"Russia warns of war within a decade over Arc...   \n",
       "5          b' Boycott Israel campaign starts to bite'   \n",
       "8   b'Canadian detectives beer knowledge leads to ...   \n",
       "9   b\"'Ghost airports' - South Korea's new airport...   \n",
       "10  b'Thousands beaten, raped in Irish reform scho...   \n",
       "11  b'A 15-year-old girl has been charged with cul...   \n",
       "\n",
       "                                                 Top5  \\\n",
       "0   b\"Without a Moral Leg to Stand On; United Arab...   \n",
       "1   b\"India's Election: how democracy may turn the...   \n",
       "2   b'Video of a Guatemalan lawyer filmed the day ...   \n",
       "3   b'Five men beheaded in Saudi Arabia after gros...   \n",
       "4   b'40,000-year-old figurine thought to be the o...   \n",
       "5   b'\"If you are watching this message, it is bec...   \n",
       "8   b'I give it 72 hours tops before 4chan has hac...   \n",
       "9   b'At least 153 of the newly-elected MPs in Ind...   \n",
       "10  b'Thousands Rally Against President Colom in G...   \n",
       "11  b\"How fox news' sister channel, sky news, deal...   \n",
       "\n",
       "                                                 Top6  \\\n",
       "0   b\"Meet Noemi Letizia, the 18 year old hottie b...   \n",
       "1   b'Dr. Kamal Labwani, a prominent political and...   \n",
       "2   b'The World Health Organization is investigati...   \n",
       "3   b'The Vatican made a surprise denial today tha...   \n",
       "4   b'Gaza man: Israeli troops forced me to drink ...   \n",
       "5   b'Muslim famous for predicting a totally Islam...   \n",
       "8   b'Banks like Citigroup-owned Banamex get away ...   \n",
       "9   b'Gordon Brown announces any MP who \"defied th...   \n",
       "10  b'Netanyahu aide call\\'s Obama\\'s 2-state plan...   \n",
       "11  b'Vancouver 2010 Olympic torch is...uniquely C...   \n",
       "\n",
       "                                                 Top7  \\\n",
       "0   b'So while everyone was fearing the Swine Flu ...   \n",
       "1   b'\"We want to leave the city, but we cannot go...   \n",
       "2                   b'Cocaine Purity down, Prices up'   \n",
       "3   b\"Sri Lanka has written off thousands of its o...   \n",
       "4        b'Russia warns of war over Arctic resources'   \n",
       "5   b'Australian immigration authorities told a de...   \n",
       "8      b\" Sabotage hits Britain's biggest cycle race\"   \n",
       "9   b'Sri Lanka: Chronology of the key dates in th...   \n",
       "10  b'Quarter of a million Sri Lankans face two ye...   \n",
       "11    b\"UN Gaza inquiry 'to proceed despite Israel' \"   \n",
       "\n",
       "                                                 Top8  ...  \\\n",
       "0   b'Defense Minister Ehud Barak said he would ca...  ...   \n",
       "1   b'Afghanistan: NATO Should Come Clean on White...  ...   \n",
       "2   b'Guatemala: In YouTube Video Shot Before His ...  ...   \n",
       "3   b'Guatemalan president accused of murder in vi...  ...   \n",
       "4   b\"Gordon Brown's government has repeatedly bla...  ...   \n",
       "5   b'Guatemala arrests Twitter user for inciting ...  ...   \n",
       "8   b'Leading Muslim Scholars in Pakistan Denounce...  ...   \n",
       "9   b'Media coverage comparison of the conflicts i...  ...   \n",
       "10  b'Owls and kestrels are being employed as agri...  ...   \n",
       "11  b' First Italian restaurant opens in Pyongyang...  ...   \n",
       "\n",
       "                                                Top19  \\\n",
       "0   b'An interview with a jailed Somali pirate lea...   \n",
       "1   b\"1 million people now displaced by fighting i...   \n",
       "2   b'Sri Lankan government forces shells hospital...   \n",
       "3   b\"Roxana Saberi, the journalist recently relea...   \n",
       "4   b'British Association for Adoption and Fosteri...   \n",
       "5   b\"Shadowy Bilderberg group meet in Greece--and...   \n",
       "8             b'Australian topless bathing ban urged'   \n",
       "9   b'Leader of the Tamil Tigers killed. It really...   \n",
       "10  b'Iran test-fired a new advanced missile Wedne...   \n",
       "11  b'The Poor Get Poorer: While Americans are wor...   \n",
       "\n",
       "                                                Top20  \\\n",
       "0   b'Delara Darabi: \"Oh mother, I see the hangman...   \n",
       "1   b'Sikh police in Britain push for bulletproof ...   \n",
       "2   b\"The man Netanyahu plans to make Israel's new...   \n",
       "3   b'Pope tells Palestinians: \"the wall that intr...   \n",
       "4   b'Finland ignored warnings of prisoner prostit...   \n",
       "5   b'Vietnam veteran on spiritual quest lands Aun...   \n",
       "8   b'Guatemala: Thousands protest over allegation...   \n",
       "9   b'Ethiopian military forces have crossed back ...   \n",
       "10  b'Silvio Berlusconi, \"Basically, we just don\\'...   \n",
       "11  b'Burma bars international observers from the ...   \n",
       "\n",
       "                                                Top21  \\\n",
       "0   b'SARS sleuth attacks WHO for being worthless ...   \n",
       "1   b'Gunmen kill Darfur peacekeeper during carjac...   \n",
       "2   b'Doctor says 49 killed in Sri Lanka hospital ...   \n",
       "3   b'Kremlin: Battles over energy may lead to war...   \n",
       "4   b'Men should hold onto dongs... [Seriously, th...   \n",
       "5   b'Avigdor Lieberman - To ban Israeli Arabs fro...   \n",
       "8   b'James Lull, Ponzi Scammer, Drives Truck Off ...   \n",
       "9   b'Careful, sober analysis of situation in Guat...   \n",
       "10  b'Sri Lanka crisis deepens as Red Cross suspen...   \n",
       "11  b'Movie and music industry appeals Pirate Bay ...   \n",
       "\n",
       "                                                Top22  \\\n",
       "0   b'Targeted by Taliban, Sikhs flee Pak region; ...   \n",
       "1   b'Saudi beauty queen: Veiled face and judged o...   \n",
       "2           b'49 killed in Sri Lanka hospital attack'   \n",
       "3   b'MDC activist dies from ZANU-PF militia tortu...   \n",
       "4   b'Obsession with Naked Women Dates Back 35,000...   \n",
       "5   b'Paul Craig Roberts on the Global Anti-Semiti...   \n",
       "8   b'Darfur rebel leader surrenders to war crimes...   \n",
       "9                 b\"Sri Lanka leader hails 'victory'\"   \n",
       "10  b'Spanish lawmakers move to curb foreign human...   \n",
       "11  b'Lucky couple - Gone missing after bank mista...   \n",
       "\n",
       "                                                Top23  \\\n",
       "0   b'Israel Faces a Hard Sell: \"The American, Eur...   \n",
       "1   b'Sierra Leone child miners: Legacy of conflict '   \n",
       "2   b\"This is London  the capital of Somali pirate...   \n",
       "3   b'Somalia:  Islamist fighters poised to topple...   \n",
       "4   b\"Burma's bad ass ruling military junta: They ...   \n",
       "5               b'The rich, shadowy Bilderberg group'   \n",
       "8           b'Woman, 66, defends right to have baby!'   \n",
       "9   b'Young Pakistanis Take One Problem Into Their...   \n",
       "10  b'Sri Lanka on brink of catastrophe as UN aid ...   \n",
       "11               b'Immigrants hurt in Greek violence'   \n",
       "\n",
       "                                                Top24  \\\n",
       "0   b'French Navy said they seized 11 pirates Sund...   \n",
       "1   b'CODEPINK Mothers Day 2009: \"I will not raise...   \n",
       "2   b'Russia holds Victory Day celebration: rows o...   \n",
       "3   b'Data-Mining Predicted Swine Flu 18 Days befo...   \n",
       "4   b\"Saudi Judge Causes Uproar For Saying It's Ok...   \n",
       "5   b\"The World's Best Illusion: The Secret of the...   \n",
       "8             b'Indian markets surge on Congress win'   \n",
       "9   b'Thailand:  Tourist faces between 2 and 5 yea...   \n",
       "10      b'Myanmar to let diplomats meet with Suu Kyi'   \n",
       "11               b\"US intruder said Suu Kyi 'scared'\"   \n",
       "\n",
       "                                                Top25 Label Label_4part  \\\n",
       "0   b'More Gaza patients quizzed at crossings, pre...   1.0         3.0   \n",
       "1   b'Gunman shoots three police, holed up in hous...   1.0         3.0   \n",
       "2     b'Torture: It Was Never Just Sleep Deprivation'   1.0         2.0   \n",
       "3   b\"Scientists in Bolivia say that one of the co...   0.0         0.0   \n",
       "4   b'Swine flu infected Mexico\\'s relations with ...   1.0         2.0   \n",
       "5                          b\"Defining 'Success' Down\"   0.0         1.0   \n",
       "8   b'The hijacking of merchant ships by Somali pi...   1.0         3.0   \n",
       "9   b'MPs on all sides tell the Speaker Michael Ma...   0.0         1.0   \n",
       "10  b'Anti-Semitic protestors in Argentina give pr...   0.0         1.0   \n",
       "11  b'Thousands raped and abused in Catholic schools'   0.0         0.0   \n",
       "\n",
       "   Label_3part  \n",
       "0          2.0  \n",
       "1          2.0  \n",
       "2          2.0  \n",
       "3          0.0  \n",
       "4          2.0  \n",
       "5          0.0  \n",
       "8          2.0  \n",
       "9          0.0  \n",
       "10         0.0  \n",
       "11         0.0  \n",
       "\n",
       "[10 rows x 30 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('./input/Combined_News_DJIA.csv')\n",
    "tweets = pd.read_csv('./input/realdonaldtrump.csv')\n",
    "tweets[\"Date\"] = pd.to_datetime(tweets[\"Date\"])\n",
    "tweets['Date']=tweets['Date'].dt.strftime('%Y-%m-%d')\n",
    "tweets = tweets.groupby('Date')['Tweet'].apply(', '.join).reset_index()\n",
    "tweets['Top26'] = tweets['Tweet'].str.replace('http\\S+|www.\\S+', ' ', case=False)\n",
    "tweets.to_csv('./output/realdonaldtrump-cooked.csv')\n",
    "tweets.drop(columns=[\"Tweet\"], inplace=True)\n",
    "data = pd.merge(tweets, data,how='left',on='Date')\n",
    "data.drop(columns=[\"Label\"], inplace=True)\n",
    "\n",
    "#add more labels\n",
    "more_labels = pd.read_csv('./input/DJIA_table_all_addlabel.csv')\n",
    "# binary label\n",
    "more_labels['Label']=np.where(more_labels['Open'] < more_labels['Close'], 1.0, 0.0)\n",
    "# label_4part\n",
    "#more_labels['Label'] = more_labels['Label_4part']\n",
    "# label_3part\n",
    "#more_labels['Label'] = more_labels['Label_3part']\n",
    "binaryLabel = more_labels[['Date', 'Label', 'Label_4part', 'Label_3part']]\n",
    "binaryLabel[\"Date\"] = pd.to_datetime(binaryLabel[\"Date\"])\n",
    "binaryLabel['Date']=binaryLabel['Date'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "data = pd.merge(data, binaryLabel,how='left',on='Date')\n",
    "\n",
    "# Drop rows with any empty cells\n",
    "data.dropna(\n",
    "    axis=0,\n",
    "    how='any',\n",
    "    thresh=None,\n",
    "    subset=None,\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "#data.head(10)\n",
    "\n",
    "#train = data[data['Date'] < '2015-05-31']\n",
    "#test = data[data['Date'] > '2015-06-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Tweet1</th>\n",
       "      <th>Label</th>\n",
       "      <th>Label_3part</th>\n",
       "      <th>Label_4part</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2012-01-10</td>\n",
       "      <td>My @ FoxNews interview with @ gretawire where ...</td>\n",
       "      <td>My @ FoxNews interview with @ gretawire where ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2013-01-10</td>\n",
       "      <td>\"Money may not grow from trees, but it does gr...</td>\n",
       "      <td>\"Money may not grow from trees, but it does gr...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2014-01-10</td>\n",
       "      <td>\" @ Discoboy7 Salmond is on a one man crusade ...</td>\n",
       "      <td>\" @ Discoboy7 Salmond is on a one man crusade ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2017-01-10</td>\n",
       "      <td>'U.S. Small-Business Optimism Index Surges by ...</td>\n",
       "      <td>'U.S. Small-Business Optimism Index Surges by ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2018-01-10</td>\n",
       "      <td>It just shows everyone how broken and unfair o...</td>\n",
       "      <td>It just shows everyone how broken and unfair o...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2019-01-10</td>\n",
       "      <td>Cryin Chuck told his favorite lie when he used...</td>\n",
       "      <td>Cryin Chuck told his favorite lie when he used...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2012-01-11</td>\n",
       "      <td>\"President Reagan had it right: Social Securit...</td>\n",
       "      <td>\"President Reagan had it right: Social Securit...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2013-01-11</td>\n",
       "      <td>@Spot_O_TEA Thanks., @ Melbelle96 Thanks Melan...</td>\n",
       "      <td>@Spot_O_TEA Thanks., @ Melbelle96 Thanks Melan...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2016-01-11</td>\n",
       "      <td>Why does @ ThisWeekABC w/ @ GStephanopoulos al...</td>\n",
       "      <td>Why does @ ThisWeekABC w/ @ GStephanopoulos al...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2017-01-11</td>\n",
       "      <td>Russia just said the unverified report paid fo...</td>\n",
       "      <td>Russia just said the unverified report paid fo...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date                                              Tweet  \\\n",
       "7   2012-01-10  My @ FoxNews interview with @ gretawire where ...   \n",
       "8   2013-01-10  \"Money may not grow from trees, but it does gr...   \n",
       "9   2014-01-10  \" @ Discoboy7 Salmond is on a one man crusade ...   \n",
       "12  2017-01-10  'U.S. Small-Business Optimism Index Surges by ...   \n",
       "13  2018-01-10  It just shows everyone how broken and unfair o...   \n",
       "14  2019-01-10  Cryin Chuck told his favorite lie when he used...   \n",
       "16  2012-01-11  \"President Reagan had it right: Social Securit...   \n",
       "17  2013-01-11  @Spot_O_TEA Thanks., @ Melbelle96 Thanks Melan...   \n",
       "20  2016-01-11  Why does @ ThisWeekABC w/ @ GStephanopoulos al...   \n",
       "21  2017-01-11  Russia just said the unverified report paid fo...   \n",
       "\n",
       "                                               Tweet1  Label  Label_3part  \\\n",
       "7   My @ FoxNews interview with @ gretawire where ...    1.0          2.0   \n",
       "8   \"Money may not grow from trees, but it does gr...    1.0          2.0   \n",
       "9   \" @ Discoboy7 Salmond is on a one man crusade ...    0.0          1.0   \n",
       "12  'U.S. Small-Business Optimism Index Surges by ...    0.0          1.0   \n",
       "13  It just shows everyone how broken and unfair o...    1.0          1.0   \n",
       "14  Cryin Chuck told his favorite lie when he used...    1.0          2.0   \n",
       "16  \"President Reagan had it right: Social Securit...    0.0          1.0   \n",
       "17  @Spot_O_TEA Thanks., @ Melbelle96 Thanks Melan...    1.0          1.0   \n",
       "20  Why does @ ThisWeekABC w/ @ GStephanopoulos al...    1.0          1.0   \n",
       "21  Russia just said the unverified report paid fo...    1.0          2.0   \n",
       "\n",
       "    Label_4part  \n",
       "7           2.0  \n",
       "8           2.0  \n",
       "9           1.0  \n",
       "12          1.0  \n",
       "13          2.0  \n",
       "14          2.0  \n",
       "16          1.0  \n",
       "17          2.0  \n",
       "20          2.0  \n",
       "21          2.0  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add Trump tweet\n",
    "tweets = pd.read_csv('./input/realdonaldtrump.csv')\n",
    "tweets = tweets.groupby('Date')['Tweet'].apply(', '.join).reset_index()\n",
    "#tweets.head(20)\n",
    "#remove URLs in tweet\n",
    "tweets[\"Date\"] = pd.to_datetime(tweets[\"Date\"])\n",
    "tweets[\"Date\"]=tweets[\"Date\"].dt.strftime('%Y-%m-%d')\n",
    "tweets['Tweet'] = tweets['Tweet'].str.replace('http\\S+|www.\\S+', ' ', case=False)\n",
    "tweets['Tweet1'] = tweets['Tweet']\n",
    "tweets.head(10)\n",
    "tweets.to_csv('./output/realdonaldtrump-cooked.csv')\n",
    "\n",
    "tweets = pd.merge(tweets, binaryLabel[['Date','Label', 'Label_3part','Label_4part']],how='left',on='Date')\n",
    "# Drop rows with any empty cells\n",
    "tweets.dropna(\n",
    "    axis=0,\n",
    "    how='any',\n",
    "    thresh=None,\n",
    "    subset=None,\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "train = tweets[tweets['Date'] < '2019-05-31']\n",
    "test = tweets[tweets['Date'] > '2019-06-01']\n",
    "\n",
    "#train.head(10)\n",
    "\n",
    "#tweets.drop(columns=[\"Tweet\"], inplace=True)\n",
    "#tweetsTrain = tweets[tweets['Date'] < '2015-05-31']\n",
    "#tweetsTest = tweets[tweets['Date'] > '2015-06-01']\n",
    "\n",
    "#train = pd.merge(train, tweetsTrain[['Date','Top26']],how='left',on='Date')\n",
    "#test = pd.merge(test, tweetsTest[['Date','Top26']],how='left',on='Date')\n",
    "#data.to_csv('./output/combined-tweets.csv')\n",
    "#data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3d372c26-3c4c-7890-3089-c8ddb17ca81a"
   },
   "source": [
    "# Data Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b43212e0-9344-a216-b86b-37cfbfb37752"
   },
   "source": [
    "First, we transform the string of news into the  number of words as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_cell_guid": "a87f0d3d-80f3-af90-9368-dedde8c1154f"
   },
   "outputs": [],
   "source": [
    "trainheadlines = []\n",
    "for row in range(0,len(train.index)):\n",
    "    trainheadlines.append(' '.join(str(x) for x in train.iloc[row,1:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_cell_guid": "66913298-12f3-da89-030a-90ba38b1e11b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2195, 31549)\n"
     ]
    }
   ],
   "source": [
    "basicvectorizer = CountVectorizer()\n",
    "basictrain = basicvectorizer.fit_transform(trainheadlines)\n",
    "print(basictrain.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4647ab33-fd7e-0d98-9afd-7ea21eb02cf8"
   },
   "source": [
    "## Logic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ee566614-2f26-df99-3c54-700a07cf83bd"
   },
   "source": [
    "### Logic Regression 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "dc0ac1c7-f4fe-3bbe-18fc-f578947626b1"
   },
   "source": [
    "Algorithm: Logic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5a6e98cc-ec7f-3dd2-5dce-2c2c04cff260"
   },
   "source": [
    "Input: the counts of single words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "_cell_guid": "5adfb2da-7467-505b-d44b-0a028e60227b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\daih\\Anaconda3\\envs\\st\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "basicmodel = LogisticRegression()\n",
    "basicmodel = basicmodel.fit(basictrain, train[\"Label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_cell_guid": "56d1ea73-641b-6fdf-698f-32518fe99dec"
   },
   "outputs": [],
   "source": [
    "testheadlines = []\n",
    "for row in range(0,len(test.index)):\n",
    "    testheadlines.append(' '.join(str(x) for x in test.iloc[row,1:2]))\n",
    "basictest = basicvectorizer.transform(testheadlines)\n",
    "preds1 = basicmodel.predict(basictest)\n",
    "acc1=accuracy_score(test['Label'], preds1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_cell_guid": "bc13f327-70bd-2f8f-4ab7-6a9dfae356e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logic Regression 1 accuracy:  0.3574144486692015\n"
     ]
    }
   ],
   "source": [
    "print('Logic Regression 1 accuracy: ',acc1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c08a6f88-d723-97b5-dd09-a61b1f3f7829"
   },
   "source": [
    "The accuracy is only 0.42.\n",
    "\n",
    "Trump tweets as only source\n",
    "0.51 accuracy if we use 2019/05/31 train/test data\n",
    "0.49 accuracy if we use 2018/05/31 train/test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "_cell_guid": "2303559d-4ad8-7d30-ff3d-ee3bb91440ae"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22486</th>\n",
       "      <td>questions</td>\n",
       "      <td>0.589787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6841</th>\n",
       "      <td>could</td>\n",
       "      <td>0.554146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12995</th>\n",
       "      <td>hope</td>\n",
       "      <td>0.508515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30626</th>\n",
       "      <td>wife</td>\n",
       "      <td>0.475421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30299</th>\n",
       "      <td>waste</td>\n",
       "      <td>0.467966</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Word  Coefficient\n",
       "22486  questions     0.589787\n",
       "6841       could     0.554146\n",
       "12995       hope     0.508515\n",
       "30626       wife     0.475421\n",
       "30299      waste     0.467966"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basicwords = basicvectorizer.get_feature_names()\n",
    "basiccoeffs = basicmodel.coef_.tolist()[0]\n",
    "coeffdf = pd.DataFrame({'Word' : basicwords, \n",
    "                        'Coefficient' : basiccoeffs})\n",
    "coeffdf = coeffdf.sort_values(['Coefficient', 'Word'], ascending=[0, 1])\n",
    "coeffdf.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "_cell_guid": "949e33a5-f63d-0059-2ee0-17e625aede36"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16687</th>\n",
       "      <td>lie</td>\n",
       "      <td>-0.514485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26876</th>\n",
       "      <td>success</td>\n",
       "      <td>-0.539553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18137</th>\n",
       "      <td>melania</td>\n",
       "      <td>-0.544801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7517</th>\n",
       "      <td>david</td>\n",
       "      <td>-0.577636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8201</th>\n",
       "      <td>did</td>\n",
       "      <td>-0.621151</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Word  Coefficient\n",
       "16687      lie    -0.514485\n",
       "26876  success    -0.539553\n",
       "18137  melania    -0.544801\n",
       "7517     david    -0.577636\n",
       "8201       did    -0.621151"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coeffdf.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6aa2253b-0cf6-4853-4ff5-6143dae71fb1"
   },
   "source": [
    "### Logic Regression 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "65239831-43f2-290b-8e21-b4a9f82e5a54"
   },
   "source": [
    "Algorithm: Logic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a108baea-371d-d93f-7d65-213ecd4337ec"
   },
   "source": [
    "Input: the counts of phrases with two connected words(exclude words which are too common like \"a\" ,\"an\" ,\"the\" and words too uncommon of which counts are too small )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "15df5336-f6f9-cd66-b020-6072aff3083b"
   },
   "source": [
    "We delete phrases of which frequency lower than 0.03 or higher than 0.97"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "_cell_guid": "dbdf7b4f-4673-49f1-d510-377026c95611"
   },
   "outputs": [],
   "source": [
    "advancedvectorizer = TfidfVectorizer( min_df=0.03, max_df=0.97, max_features = 200000, ngram_range = (2, 2))\n",
    "advancedtrain = advancedvectorizer.fit_transform(trainheadlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "_cell_guid": "b6bf747c-021c-eb7f-cd50-2240138ed555"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2195, 412)\n"
     ]
    }
   ],
   "source": [
    "print(advancedtrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "_cell_guid": "8c225d22-d997-5db8-877e-03022a0785b4"
   },
   "outputs": [],
   "source": [
    "advancedmodel = LogisticRegression()\n",
    "advancedmodel = advancedmodel.fit(advancedtrain, train[\"Label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "_cell_guid": "1b3421b9-b326-4ef5-4235-2e764e5cd3a3"
   },
   "outputs": [],
   "source": [
    "testheadlines = []\n",
    "for row in range(0,len(test.index)):\n",
    "    testheadlines.append(' '.join(str(x) for x in test.iloc[row,1:2]))\n",
    "advancedtest = advancedvectorizer.transform(testheadlines)\n",
    "preds2 = advancedmodel.predict(advancedtest)\n",
    "acc2=accuracy_score(test['Label'], preds2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "_cell_guid": "422fb025-f985-e010-3e9e-2ee8c4494c3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logic Regression 2 accuracy:  0.3155893536121673\n"
     ]
    }
   ],
   "source": [
    "print('Logic Regression 2 accuracy: ', acc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8f5a4ca4-75a0-6a24-2c34-61855d2b102a"
   },
   "source": [
    "The accuracy is higher than input of single words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "_cell_guid": "3af2a0fb-a8ef-84f2-2a80-3568d327baec"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Words</th>\n",
       "      <th>Coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>they can</td>\n",
       "      <td>1.068292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>going on</td>\n",
       "      <td>1.009603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>in my</td>\n",
       "      <td>0.892559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>to help</td>\n",
       "      <td>0.804999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>to stop</td>\n",
       "      <td>0.799860</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Words  Coefficient\n",
       "303  they can     1.068292\n",
       "83   going on     1.009603\n",
       "121     in my     0.892559\n",
       "324   to help     0.804999\n",
       "333   to stop     0.799860"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "advwords = advancedvectorizer.get_feature_names()\n",
    "advcoeffs = advancedmodel.coef_.tolist()[0]\n",
    "advcoeffdf = pd.DataFrame({'Words' : advwords, \n",
    "                        'Coefficient' : advcoeffs})\n",
    "advcoeffdf = advcoeffdf.sort_values(['Coefficient', 'Words'], ascending=[0, 1])\n",
    "advcoeffdf.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "_cell_guid": "b1fd8c2b-23fd-7dda-cf8d-0381c3d0736f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Words</th>\n",
       "      <th>Coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>not the</td>\n",
       "      <td>-0.855420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>you to</td>\n",
       "      <td>-0.898693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>this morning</td>\n",
       "      <td>-0.907430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>for our</td>\n",
       "      <td>-1.077052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>as the</td>\n",
       "      <td>-1.125570</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Words  Coefficient\n",
       "182       not the    -0.855420\n",
       "407        you to    -0.898693\n",
       "313  this morning    -0.907430\n",
       "69        for our    -1.077052\n",
       "27         as the    -1.125570"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "advcoeffdf.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "39b0d7a4-cedc-d51c-11f4-d3a1d454682f"
   },
   "source": [
    "### Logic Regression 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4cf79ce1-930d-474a-f07b-dd397f9a72eb"
   },
   "source": [
    "Algorithm: Logic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a62cefba-3f14-ce21-49a5-1fdd09556dde"
   },
   "source": [
    "Input: the counts of phrases with three connected words(exclude words which are too common like \"a\" ,\"an\" ,\"the\" and words too uncommon of which counts are too small )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "_cell_guid": "886f33fe-d47f-d545-1b73-29f1e3618e01"
   },
   "outputs": [],
   "source": [
    "advancedvectorizer = TfidfVectorizer( min_df=0.0039, max_df=0.1, max_features = 200000, ngram_range = (3, 3))\n",
    "advancedtrain = advancedvectorizer.fit_transform(trainheadlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "_cell_guid": "5437284a-b14e-96cd-e6d5-97c41790d42a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2195, 1981)\n"
     ]
    }
   ],
   "source": [
    "print(advancedtrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "_cell_guid": "5e29bfca-0041-5b86-fdff-9c502f4aa1ad"
   },
   "outputs": [],
   "source": [
    "advancedmodel = LogisticRegression()\n",
    "advancedmodel = advancedmodel.fit(advancedtrain, train[\"Label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "_cell_guid": "d5717cfb-3f8c-1af4-665b-6a22e6e9c9ba"
   },
   "outputs": [],
   "source": [
    "testheadlines = []\n",
    "for row in range(0,len(test.index)):\n",
    "    testheadlines.append(' '.join(str(x) for x in test.iloc[row,1:2]))\n",
    "advancedtest = advancedvectorizer.transform(testheadlines)\n",
    "preds3 = advancedmodel.predict(advancedtest)\n",
    "acc3 = accuracy_score(test['Label'], preds3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "_cell_guid": "b700896c-936b-004d-2d70-50fefee2129c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logic Regression 3 accuracy:  0.3726235741444867\n"
     ]
    }
   ],
   "source": [
    "print('Logic Regression 3 accuracy: ', acc3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2a438254-8acb-4db7-4f38-bb07be962dc2"
   },
   "source": [
    "The accuracy is lower than input of phrases with two connected words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "_cell_guid": "66fee024-03b4-5964-f925-cfb3746a4741"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Words</th>\n",
       "      <th>Coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1208</th>\n",
       "      <td>should not be</td>\n",
       "      <td>1.060529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1234</th>\n",
       "      <td>squawkcnbc interview discussing</td>\n",
       "      <td>0.993034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1046</th>\n",
       "      <td>our leaders are</td>\n",
       "      <td>0.972492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1363</th>\n",
       "      <td>the first time</td>\n",
       "      <td>0.954336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1961</th>\n",
       "      <td>you think you</td>\n",
       "      <td>0.902798</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Words  Coefficient\n",
       "1208                    should not be     1.060529\n",
       "1234  squawkcnbc interview discussing     0.993034\n",
       "1046                  our leaders are     0.972492\n",
       "1363                   the first time     0.954336\n",
       "1961                    you think you     0.902798"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "advwords = advancedvectorizer.get_feature_names()\n",
    "advcoeffs = advancedmodel.coef_.tolist()[0]\n",
    "advcoeffdf = pd.DataFrame({'Words' : advwords, \n",
    "                        'Coefficient' : advcoeffs})\n",
    "advcoeffdf = advcoeffdf.sort_values(['Coefficient', 'Words'], ascending=[0, 1])\n",
    "advcoeffdf.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "_cell_guid": "c50f97d8-a6f7-0a60-9edf-42bb8b7aa492"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Words</th>\n",
       "      <th>Coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1630</th>\n",
       "      <td>tomorrow night at</td>\n",
       "      <td>-0.695695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>and their families</td>\n",
       "      <td>-0.698122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1357</th>\n",
       "      <td>the failing nytimes</td>\n",
       "      <td>-0.827777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1248</th>\n",
       "      <td>take care of</td>\n",
       "      <td>-0.859959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>on behalf of</td>\n",
       "      <td>-0.882792</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Words  Coefficient\n",
       "1630    tomorrow night at    -0.695695\n",
       "76     and their families    -0.698122\n",
       "1357  the failing nytimes    -0.827777\n",
       "1248         take care of    -0.859959\n",
       "977          on behalf of    -0.882792"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "advcoeffdf.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a4229efa-49d5-82fb-5a1d-e1ca4001e0ac"
   },
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5ccc2e71-28c6-f425-f229-f1395293bffc"
   },
   "source": [
    "### NBayes 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "_cell_guid": "476ba54c-4b47-6029-e0de-29e6fcb64845"
   },
   "outputs": [],
   "source": [
    "advancedvectorizer = TfidfVectorizer( min_df=0.1, max_df=0.7, max_features = 200000, ngram_range = (1, 1))\n",
    "advancedtrain = advancedvectorizer.fit_transform(trainheadlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "_cell_guid": "1ce37cf1-5c09-9fe7-09ec-a23d6d3b749e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2195, 242)\n"
     ]
    }
   ],
   "source": [
    "print(advancedtrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "_cell_guid": "4726d110-1821-ea71-dc5c-7a21ad628dd1"
   },
   "outputs": [],
   "source": [
    "advancedmodel = MultinomialNB(alpha=0.01)\n",
    "advancedmodel = advancedmodel.fit(advancedtrain, train[\"Label\"])\n",
    "testheadlines = []\n",
    "for row in range(0,len(test.index)):\n",
    "    testheadlines.append(' '.join(str(x) for x in test.iloc[row,1:2]))\n",
    "advancedtest = advancedvectorizer.transform(testheadlines)\n",
    "preds4 = advancedmodel.predict(advancedtest)\n",
    "acc4=accuracy_score(test['Label'], preds4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "_cell_guid": "ff1f469d-1c12-5a3d-1aed-148179c26824"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NBayes 1 accuracy:  0.3269961977186312\n"
     ]
    }
   ],
   "source": [
    "print('NBayes 1 accuracy: ', acc4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "_cell_guid": "02a9958e-ab18-bf74-58f1-160c5b47cbff"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Words</th>\n",
       "      <th>Coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>you</td>\n",
       "      <td>-3.937314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>realdonaldtrump</td>\n",
       "      <td>-4.008708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>trump</td>\n",
       "      <td>-4.257856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>it</td>\n",
       "      <td>-4.352573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>great</td>\n",
       "      <td>-4.373338</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Words  Coefficient\n",
       "240              you    -3.937314\n",
       "161  realdonaldtrump    -4.008708\n",
       "204            trump    -4.257856\n",
       "101               it    -4.352573\n",
       "80             great    -4.373338"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "advwords = advancedvectorizer.get_feature_names()\n",
    "advcoeffs = advancedmodel.coef_.tolist()[0]\n",
    "advcoeffdf = pd.DataFrame({'Words' : advwords, \n",
    "                        'Coefficient' : advcoeffs})\n",
    "advcoeffdf = advcoeffdf.sort_values(['Coefficient', 'Words'], ascending=[0, 1])\n",
    "advcoeffdf.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "_cell_guid": "6223b466-48eb-0a0a-5d36-c082e50b1fe5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Words</th>\n",
       "      <th>Coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>off</td>\n",
       "      <td>-6.409500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>things</td>\n",
       "      <td>-6.411220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>smart</td>\n",
       "      <td>-6.452290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>such</td>\n",
       "      <td>-6.478699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>place</td>\n",
       "      <td>-6.527927</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Words  Coefficient\n",
       "145     off    -6.409500\n",
       "194  things    -6.411220\n",
       "172   smart    -6.452290\n",
       "181    such    -6.478699\n",
       "156   place    -6.527927"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "advcoeffdf.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "dcc814d2-014d-5cb9-b106-52ff648d9c18"
   },
   "source": [
    "### NBayes 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "_cell_guid": "dfe60202-6348-d724-c860-c3b46c66c447"
   },
   "outputs": [],
   "source": [
    "advancedvectorizer = TfidfVectorizer( min_df=0.03, max_df=0.2, max_features = 200000, ngram_range = (2, 2))\n",
    "advancedtrain = advancedvectorizer.fit_transform(trainheadlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "_cell_guid": "28b21c32-28de-1bc4-161e-dcf63737caca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2195, 398)\n"
     ]
    }
   ],
   "source": [
    "print(advancedtrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "_cell_guid": "23ab2ed6-8e9a-4291-2f9b-6fc2563f33ca"
   },
   "outputs": [],
   "source": [
    "advancedmodel = MultinomialNB(alpha=0.0001)\n",
    "advancedmodel = advancedmodel.fit(advancedtrain, train[\"Label\"])\n",
    "testheadlines = []\n",
    "for row in range(0,len(test.index)):\n",
    "    testheadlines.append(' '.join(str(x) for x in test.iloc[row,1:2]))\n",
    "advancedtest = advancedvectorizer.transform(testheadlines)\n",
    "preds5 = advancedmodel.predict(advancedtest)\n",
    "acc5 = accuracy_score(test['Label'], preds5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "_cell_guid": "aa7d48c3-60cc-0895-7ed3-ddbf796339ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NBayes 2 accuracy:  0.33840304182509506\n"
     ]
    }
   ],
   "source": [
    "print('NBayes 2 accuracy: ', acc5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "_cell_guid": "f49cb3c4-b80b-48a8-69be-ac6c579e2ed4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Words</th>\n",
       "      <th>Coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>for president</td>\n",
       "      <td>-4.954937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>and the</td>\n",
       "      <td>-4.956402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>last night</td>\n",
       "      <td>-5.011622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>we are</td>\n",
       "      <td>-5.020182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>our country</td>\n",
       "      <td>-5.024710</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Words  Coefficient\n",
       "68   for president    -4.954937\n",
       "18         and the    -4.956402\n",
       "145     last night    -5.011622\n",
       "352         we are    -5.020182\n",
       "197    our country    -5.024710"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "advwords = advancedvectorizer.get_feature_names()\n",
    "advcoeffs = advancedmodel.coef_.tolist()[0]\n",
    "advcoeffdf = pd.DataFrame({'Words' : advwords, \n",
    "                        'Coefficient' : advcoeffs})\n",
    "advcoeffdf = advcoeffdf.sort_values(['Coefficient', 'Words'], ascending=[0, 1])\n",
    "advcoeffdf.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "_cell_guid": "087f9b18-c50b-17b5-60b6-2cf2dada7c3c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Words</th>\n",
       "      <th>Coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>years ago</td>\n",
       "      <td>-6.922508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>not the</td>\n",
       "      <td>-6.925774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>the man</td>\n",
       "      <td>-6.939664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>no one</td>\n",
       "      <td>-6.978273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>season of</td>\n",
       "      <td>-6.981708</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Words  Coefficient\n",
       "381  years ago    -6.922508\n",
       "176    not the    -6.925774\n",
       "263    the man    -6.939664\n",
       "173     no one    -6.978273\n",
       "223  season of    -6.981708"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "advcoeffdf.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4bfcc58e-1ed4-18d5-a321-e58a4744dd8c"
   },
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a77b2afd-5654-aaff-74a9-0013fe4e8164"
   },
   "source": [
    "### RF 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "_cell_guid": "91fe0b03-37a0-eb19-2632-cdc04067b12f"
   },
   "outputs": [],
   "source": [
    "advancedvectorizer = TfidfVectorizer( min_df=0.01, max_df=0.99, max_features = 200000, ngram_range = (1, 1))\n",
    "advancedtrain = advancedvectorizer.fit_transform(trainheadlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "_cell_guid": "878756f6-f57d-88cb-fe82-5181c1e40901"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2195, 2144)\n"
     ]
    }
   ],
   "source": [
    "print(advancedtrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "_cell_guid": "b561af90-db7d-ef58-26a2-5721d5432615"
   },
   "outputs": [],
   "source": [
    "advancedmodel = RandomForestClassifier()\n",
    "advancedmodel = advancedmodel.fit(advancedtrain, train[\"Label\"])\n",
    "testheadlines = []\n",
    "for row in range(0,len(test.index)):\n",
    "    testheadlines.append(' '.join(str(x) for x in test.iloc[row,1:2]))\n",
    "advancedtest = advancedvectorizer.transform(testheadlines)\n",
    "preds6 = advancedmodel.predict(advancedtest)\n",
    "acc6 = accuracy_score(test['Label'], preds6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "_cell_guid": "91fe3c5f-b005-97f0-da7c-597c29521877"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF 1 accuracy:  0.30798479087452474\n"
     ]
    }
   ],
   "source": [
    "print('RF 1 accuracy: ', acc6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "76a971f3-878d-33e2-249a-a2b063aa5460"
   },
   "source": [
    "### RF 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "_cell_guid": "1db5461b-1ee9-2196-6725-66a6c2376a19"
   },
   "outputs": [],
   "source": [
    "advancedvectorizer = TfidfVectorizer( min_df=0.03, max_df=0.2, max_features = 200000, ngram_range = (2, 2))\n",
    "advancedtrain = advancedvectorizer.fit_transform(trainheadlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "_cell_guid": "e1fb8019-c981-9d18-8f54-2750387ec071"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2195, 398)\n"
     ]
    }
   ],
   "source": [
    "print(advancedtrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "_cell_guid": "bcddf034-4a10-2026-55ea-dd3d969a297b"
   },
   "outputs": [],
   "source": [
    "advancedmodel = RandomForestClassifier()\n",
    "advancedmodel = advancedmodel.fit(advancedtrain, train[\"Label\"])\n",
    "testheadlines = []\n",
    "for row in range(0,len(test.index)):\n",
    "    testheadlines.append(' '.join(str(x) for x in test.iloc[row,1:2]))\n",
    "advancedtest = advancedvectorizer.transform(testheadlines)\n",
    "preds7 = advancedmodel.predict(advancedtest)\n",
    "acc7 = accuracy_score(test['Label'], preds7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "_cell_guid": "e395e3a6-7bc8-a434-16af-be13aebbabbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF 2 accuracy:  0.33840304182509506\n"
     ]
    }
   ],
   "source": [
    "print('RF 2 accuracy: ', acc7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "0a12329a-12c7-aa90-a138-025087dd76ba"
   },
   "source": [
    "## Gradient Boosting Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ea1681ab-166f-20a6-3c29-f34f84438a36"
   },
   "source": [
    "### GBM 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "_cell_guid": "4ec11f0e-6c50-8149-93c3-acd17d1e7caa"
   },
   "outputs": [],
   "source": [
    "advancedvectorizer = TfidfVectorizer( min_df=0.1, max_df=0.9, max_features = 200000, ngram_range = (1, 1))\n",
    "advancedtrain = advancedvectorizer.fit_transform(trainheadlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "_cell_guid": "3c5d312d-555f-f353-606e-2f560631033a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2195, 250)\n"
     ]
    }
   ],
   "source": [
    "print(advancedtrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "_cell_guid": "8f9d72ff-fb8e-5d1a-6016-3ebdb76a2ffd"
   },
   "outputs": [],
   "source": [
    "advancedmodel = GradientBoostingClassifier()\n",
    "advancedmodel = advancedmodel.fit(advancedtrain, train[\"Label\"])\n",
    "testheadlines = []\n",
    "for row in range(0,len(test.index)):\n",
    "    testheadlines.append(' '.join(str(x) for x in test.iloc[row,1:2]))\n",
    "advancedtest = advancedvectorizer.transform(testheadlines)\n",
    "preds8 = advancedmodel.predict(advancedtest.toarray())\n",
    "acc8 = accuracy_score(test['Label'], preds8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "_cell_guid": "ba8ae675-7e71-e24e-69bf-99d5d19590d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBM 1 accuracy:  0.34600760456273766\n"
     ]
    }
   ],
   "source": [
    "print('GBM 1 accuracy: ', acc8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e39653db-00b9-d1e0-d82e-87105e0eba1e"
   },
   "source": [
    "### GBM 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "_cell_guid": "d35d6a94-69f4-5370-6d0f-5b98c91d2565"
   },
   "outputs": [],
   "source": [
    "advancedvectorizer = TfidfVectorizer( min_df=0.02, max_df=0.175, max_features = 200000, ngram_range = (2, 2))\n",
    "advancedtrain = advancedvectorizer.fit_transform(trainheadlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "_cell_guid": "b83bc122-5059-665a-28c1-85145787155e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2195, 765)\n"
     ]
    }
   ],
   "source": [
    "print(advancedtrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "_cell_guid": "e98ca989-15e6-b785-f282-4e59f849cd1f"
   },
   "outputs": [],
   "source": [
    "advancedmodel = GradientBoostingClassifier()\n",
    "advancedmodel = advancedmodel.fit(advancedtrain, train[\"Label\"])\n",
    "testheadlines = []\n",
    "for row in range(0,len(test.index)):\n",
    "    testheadlines.append(' '.join(str(x) for x in test.iloc[row,1:2]))\n",
    "advancedtest = advancedvectorizer.transform(testheadlines)\n",
    "preds9 = advancedmodel.predict(advancedtest.toarray())\n",
    "acc9 = accuracy_score(test['Label'], preds9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "_cell_guid": "6df5f34f-a315-c1c4-fdbe-c54127468d5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBM 2 accuracy:  0.3574144486692015\n"
     ]
    }
   ],
   "source": [
    "print('GBM 2 accuracy: ', acc9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b7bd5500-e077-8225-31b5-a9c3f1015ca5"
   },
   "source": [
    "## Stochastic Gradient Descent Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8f139d81-25c5-ba11-8924-e68b6e2e55f6"
   },
   "source": [
    "### SGDClassifier 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "_cell_guid": "d071bb2f-df6a-1657-1af8-5d31ee8e4a19"
   },
   "outputs": [],
   "source": [
    "advancedvectorizer = TfidfVectorizer( min_df=0.2, max_df=0.8, max_features = 200000, ngram_range = (1, 1))\n",
    "advancedtrain = advancedvectorizer.fit_transform(trainheadlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "_cell_guid": "24d0156a-2705-fece-001e-0426da57d383"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2195, 108)\n"
     ]
    }
   ],
   "source": [
    "print(advancedtrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "_cell_guid": "30e4b846-af7e-1eda-dade-f1550d6e043b"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'n_iter'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-80-d17a39ba8c3e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0madvancedmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSGDClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'modified_huber'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0madvancedmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madvancedmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madvancedtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Label\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtestheadlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mtestheadlines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\st\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'n_iter'"
     ]
    }
   ],
   "source": [
    "advancedmodel = SGDClassifier(loss='modified_huber', n_iter=5, random_state=0, shuffle=True)\n",
    "advancedmodel = advancedmodel.fit(advancedtrain, train[\"Label\"])\n",
    "testheadlines = []\n",
    "for row in range(0,len(test.index)):\n",
    "    testheadlines.append(' '.join(str(x) for x in test.iloc[row,1:2]))\n",
    "advancedtest = advancedvectorizer.transform(testheadlines)\n",
    "preds10 = advancedmodel.predict(advancedtest.toarray())\n",
    "acc10 = accuracy_score(test['Label'], preds10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79a91d72-1cd5-4308-9b0e-9810314cdd1a"
   },
   "outputs": [],
   "source": [
    "print('SGDClassifier 1: ', acc10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "0f46cdfe-20b2-9bc6-103a-1f44e2172789"
   },
   "source": [
    "### SGDClassifier 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8ccd7ace-7326-4dfe-b93d-a8377e1a3ba6"
   },
   "outputs": [],
   "source": [
    "advancedvectorizer = TfidfVectorizer( min_df=0.03, max_df=0.2, max_features = 200000, ngram_range = (2, 2))\n",
    "advancedtrain = advancedvectorizer.fit_transform(trainheadlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c012b95d-8575-ef34-d699-d240ade631f0"
   },
   "outputs": [],
   "source": [
    "print(advancedtrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5e4f2994-e199-976d-f54b-0e676b6b11be"
   },
   "outputs": [],
   "source": [
    "advancedmodel = SGDClassifier(loss='modified_huber', n_iter=5, random_state=0, shuffle=True)\n",
    "advancedmodel = advancedmodel.fit(advancedtrain, train[\"Label\"])\n",
    "testheadlines = []\n",
    "for row in range(0,len(test.index)):\n",
    "    testheadlines.append(' '.join(str(x) for x in test.iloc[row,1:2]))\n",
    "advancedtest = advancedvectorizer.transform(testheadlines)\n",
    "preds11 = advancedmodel.predict(advancedtest.toarray())\n",
    "acc11 = accuracy_score(test['Label'], preds11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a9d8499e-ae6a-6197-c86f-c31bf73f3f98"
   },
   "outputs": [],
   "source": [
    "print('SGDClassifier 2: ', acc11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d6aa0178-e7f3-6427-f88f-aa6e436fc87d"
   },
   "source": [
    "## Naive Bayes SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "cd1391ca-1b70-13c0-3a62-151fc84f6069"
   },
   "outputs": [],
   "source": [
    "class NBSVM(six.with_metaclass(ABCMeta, BaseEstimator, ClassifierMixin)):\n",
    "\n",
    "    def __init__(self, alpha=1.0, C=1.0, max_iter=10000):\n",
    "        self.alpha = alpha\n",
    "        self.max_iter = max_iter\n",
    "        self.C = C\n",
    "        self.svm_ = [] # fuggly\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X, y = check_X_y(X, y, 'csr')\n",
    "        _, n_features = X.shape\n",
    "\n",
    "        labelbin = LabelBinarizer()\n",
    "        Y = labelbin.fit_transform(y)\n",
    "        self.classes_ = labelbin.classes_\n",
    "        if Y.shape[1] == 1:\n",
    "            Y = np.concatenate((1 - Y, Y), axis=1)\n",
    "\n",
    "        # LabelBinarizer().fit_transform() returns arrays with dtype=np.int64.\n",
    "        # so we don't have to cast X to floating point\n",
    "        Y = Y.astype(np.float64)\n",
    "\n",
    "        # Count raw events from data\n",
    "        n_effective_classes = Y.shape[1]\n",
    "        self.class_count_ = np.zeros(n_effective_classes, dtype=np.float64)\n",
    "        self.ratios_ = np.full((n_effective_classes, n_features), self.alpha,\n",
    "                                 dtype=np.float64)\n",
    "        self._compute_ratios(X, Y)\n",
    "\n",
    "        # flugglyness\n",
    "        for i in range(n_effective_classes):\n",
    "            X_i = X.multiply(self.ratios_[i])\n",
    "            svm = LinearSVC(C=self.C, max_iter=self.max_iter)\n",
    "            Y_i = Y[:,i]\n",
    "            svm.fit(X_i, Y_i)\n",
    "            self.svm_.append(svm) \n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        n_effective_classes = self.class_count_.shape[0]\n",
    "        n_examples = X.shape[0]\n",
    "\n",
    "        D = np.zeros((n_effective_classes, n_examples))\n",
    "\n",
    "        for i in range(n_effective_classes):\n",
    "            X_i = X.multiply(self.ratios_[i])\n",
    "            D[i] = self.svm_[i].decision_function(X_i)\n",
    "        \n",
    "        return self.classes_[np.argmax(D, axis=0)]\n",
    "        \n",
    "    def _compute_ratios(self, X, Y):\n",
    "        \"\"\"Count feature occurrences and compute ratios.\"\"\"\n",
    "        if np.any((X.data if issparse(X) else X) < 0):\n",
    "            raise ValueError(\"Input X must be non-negative\")\n",
    "\n",
    "        self.ratios_ += safe_sparse_dot(Y.T, X)  # ratio + feature_occurrance_c\n",
    "        normalize(self.ratios_, norm='l1', axis=1, copy=False)\n",
    "        row_calc = lambda r: np.log(np.divide(r, (1 - r)))\n",
    "        self.ratios_ = np.apply_along_axis(row_calc, axis=1, arr=self.ratios_)\n",
    "        check_array(self.ratios_)\n",
    "        self.ratios_ = sparse.csr_matrix(self.ratios_)\n",
    "\n",
    "        #p_c /= np.linalg.norm(p_c, ord=1)\n",
    "        #ratios[c] = np.log(p_c / (1 - p_c))\n",
    "\n",
    "\n",
    "def f1_class(pred, truth, class_val):\n",
    "    n = len(truth)\n",
    "\n",
    "    truth_class = 0\n",
    "    pred_class = 0\n",
    "    tp = 0\n",
    "\n",
    "    for ii in range(0, n):\n",
    "        if truth[ii] == class_val:\n",
    "            truth_class += 1\n",
    "            if truth[ii] == pred[ii]:\n",
    "                tp += 1\n",
    "                pred_class += 1\n",
    "                continue;\n",
    "        if pred[ii] == class_val:\n",
    "            pred_class += 1\n",
    "\n",
    "    precision = tp / float(pred_class)\n",
    "    recall = tp / float(truth_class)\n",
    "\n",
    "    return (2.0 * precision * recall) / (precision + recall)\n",
    "\n",
    "\n",
    "def semeval_senti_f1(pred, truth, pos=2, neg=0): \n",
    "\n",
    "    f1_pos = f1_class(pred, truth, pos)\n",
    "    f1_neg = f1_class(pred, truth, neg)\n",
    "\n",
    "    return (f1_pos + f1_neg) / 2.0;\n",
    "\n",
    "\n",
    "def main(train_file, test_file, ngram=(1, 3)):\n",
    "    print('loading...')\n",
    "    train = pd.read_csv(train_file, delimiter='\\t', encoding='utf-8', header=0,\n",
    "                        names=['text', 'label'])\n",
    "\n",
    "    # to shuffle:\n",
    "    #train.iloc[np.random.permutation(len(df))]\n",
    "\n",
    "    test = pd.read_csv(test_file, delimiter='\\t', encoding='utf-8', header=0,\n",
    "                        names=['text', 'label'])\n",
    "\n",
    "    print('vectorizing...')\n",
    "    vect = CountVectorizer()\n",
    "    classifier = NBSVM()\n",
    "\n",
    "    # create pipeline\n",
    "    clf = Pipeline([('vect', vect), ('nbsvm', classifier)])\n",
    "    params = {\n",
    "        'vect__token_pattern': r\"\\S+\",\n",
    "        'vect__ngram_range': ngram, \n",
    "        'vect__binary': True\n",
    "    }\n",
    "    clf.set_params(**params)\n",
    "\n",
    "    #X_train = vect.fit_transform(train['text'])\n",
    "    #X_test = vect.transform(test['text'])\n",
    "\n",
    "    print('fitting...')\n",
    "    clf.fit(train['text'], train['label'])\n",
    "\n",
    "    print('classifying...')\n",
    "    pred = clf.predict(test['text'])\n",
    "   \n",
    "    print('testing...')\n",
    "    acc = accuracy_score(test['label'], pred)\n",
    "    f1 = semeval_senti_f1(pred, test['label'])\n",
    "    print('NBSVM: acc=%f, f1=%f' % (acc, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7f0e000a-ca1a-5611-0ca8-c8bc40eafecb"
   },
   "source": [
    "### NBSVM 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e054e5ea-c8be-cb98-90a1-2512b3478b6e"
   },
   "outputs": [],
   "source": [
    "advancedvectorizer = TfidfVectorizer( min_df=0.1, max_df=0.8, max_features = 200000, ngram_range = (1, 1))\n",
    "advancedtrain = advancedvectorizer.fit_transform(trainheadlines)\n",
    "print(advancedtrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b57a2b30-d69f-cf0a-05d0-8a065d5a758b"
   },
   "outputs": [],
   "source": [
    "advancedmodel = NBSVM(C=0.01)\n",
    "advancedmodel = advancedmodel.fit(advancedtrain, train[\"Label\"])\n",
    "testheadlines = []\n",
    "for row in range(0,len(test.index)):\n",
    "    testheadlines.append(' '.join(str(x) for x in test.iloc[row,1:2]))\n",
    "advancedtest = advancedvectorizer.transform(testheadlines)\n",
    "preds12 = advancedmodel.predict(advancedtest)\n",
    "acc12 = accuracy_score(test['Label'], preds12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6acdade4-665e-5c06-95af-3e5ddcfd33d6"
   },
   "outputs": [],
   "source": [
    "print('NBSVM 1: ', acc12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "aca410f2-118d-f973-6b14-a63048444f26"
   },
   "source": [
    "### NBSVM 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "663e5e98-7ccd-29a4-40a8-10a46327c06b"
   },
   "outputs": [],
   "source": [
    "advancedvectorizer = TfidfVectorizer( min_df=0.031, max_df=0.2, max_features = 200000, ngram_range = (2, 2))\n",
    "advancedtrain = advancedvectorizer.fit_transform(trainheadlines)\n",
    "print(advancedtrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0c67fed4-8110-2a52-bb33-1da6f294b592"
   },
   "outputs": [],
   "source": [
    "advancedmodel = NBSVM(C=0.01)\n",
    "advancedmodel = advancedmodel.fit(advancedtrain, train[\"Label\"])\n",
    "testheadlines = []\n",
    "for row in range(0,len(test.index)):\n",
    "    testheadlines.append(' '.join(str(x) for x in test.iloc[row,1:2]))\n",
    "advancedtest = advancedvectorizer.transform(testheadlines)\n",
    "preds13 = advancedmodel.predict(advancedtest)\n",
    "acc13 = accuracy_score(test['Label'], preds13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "803861e3-d740-c2ed-2b2e-57221ba9b8c8"
   },
   "outputs": [],
   "source": [
    "print('NBSVM 2: ', acc13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "682e897f-169c-8694-d05d-66e9fc844158"
   },
   "source": [
    "## Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1de5a061-52d5-2578-9cb0-2602499ad87b"
   },
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ac32628f-8080-5d53-e3a2-3d6986431f15"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "nb_classes = 2\n",
    "advancedvectorizer = TfidfVectorizer( min_df=0.04, max_df=0.3, max_features = 200000, ngram_range = (2, 2))\n",
    "advancedtrain = advancedvectorizer.fit_transform(trainheadlines)\n",
    "testheadlines = []\n",
    "for row in range(0,len(test.index)):\n",
    "    testheadlines.append(' '.join(str(x) for x in test.iloc[row,1:2]))\n",
    "advancedtest = advancedvectorizer.transform(testheadlines)\n",
    "print(advancedtrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "eca75f7e-4fe5-0be4-85ee-47fecf3ad4d8"
   },
   "outputs": [],
   "source": [
    "X_train = advancedtrain.toarray()\n",
    "X_test = advancedtest.toarray()\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "y_train = np.array(train[\"Label\"])\n",
    "y_test = np.array(test[\"Label\"])\n",
    "\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "\n",
    "# pre-processing: divide by max and substract mean\n",
    "scale = np.max(X_train)\n",
    "X_train /= scale\n",
    "X_test /= scale\n",
    "\n",
    "mean = np.mean(X_train)\n",
    "X_train -= mean\n",
    "X_test -= mean\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "\n",
    "# Here's a Deep Dumb MLP (DDMLP)\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=input_dim))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# we'll use categorical xent for the loss, and RMSprop as the optimizer\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "print(\"Training...\")\n",
    "model.fit(X_train, Y_train, nb_epoch=2, batch_size=16, validation_split=0.15, show_accuracy=True)\n",
    "\n",
    "print(\"Generating test predictions...\")\n",
    "preds14 = model.predict_classes(X_test, verbose=0)\n",
    "acc14 = accuracy_score(test[\"Label\"], preds14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "39345ebd-2cf2-8332-6fb2-d00bc801af57"
   },
   "outputs": [],
   "source": [
    "print('prediction accuracy: ', acc14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "143c5d6d-4259-c945-f1dd-b07a4c388526"
   },
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "db232b70-f3a0-0952-e7ab-9c0b768e8025"
   },
   "outputs": [],
   "source": [
    "max_features = 10000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.1\n",
    "maxlen = 200\n",
    "batch_size = 32\n",
    "nb_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e5f6e70a-325d-5a94-0c16-c852f951d475"
   },
   "outputs": [],
   "source": [
    "# vectorize the text samples into a 2D integer tensor\n",
    "tokenizer = Tokenizer(nb_words=max_features)\n",
    "tokenizer.fit_on_texts(trainheadlines)\n",
    "sequences_train = tokenizer.texts_to_sequences(trainheadlines)\n",
    "sequences_test = tokenizer.texts_to_sequences(testheadlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a9224d6a-3bca-ef84-9817-a72b41241f54"
   },
   "outputs": [],
   "source": [
    "print('Pad sequences (samples x time)')\n",
    "X_train = sequence.pad_sequences(sequences_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(sequences_test, maxlen=maxlen)\n",
    "\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "29359f0d-8d42-2038-c04f-2cd5bc87bc93"
   },
   "outputs": [],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128, dropout=0.2))\n",
    "model.add(LSTM(128, dropout_W=0.2, dropout_U=0.2)) \n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=3,\n",
    "          validation_data=(X_test, Y_test))\n",
    "score, acc = model.evaluate(X_test, Y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)\n",
    "\n",
    "\n",
    "print(\"Generating test predictions...\")\n",
    "preds15 = model.predict_classes(X_test, verbose=0)\n",
    "acc15 = accuracy_score(test['Label'], preds15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "365d4d4b-57cd-c678-6e62-133aa32586fb"
   },
   "outputs": [],
   "source": [
    "print('prediction accuracy: ', acc15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8afb0d07-7d0d-dd18-b19c-be603f301b52"
   },
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "7271a1d1-cde0-a26c-de99-edfb7dfbf918"
   },
   "outputs": [],
   "source": [
    "nb_filter = 120\n",
    "filter_length = 2\n",
    "hidden_dims = 120\n",
    "nb_epoch = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c071e2fc-2106-4bc5-2e2f-dde4a84dabdf"
   },
   "outputs": [],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128, dropout=0.2))\n",
    "# we add a Convolution1D, which will learn nb_filter\n",
    "# word group filters of size filter_length:\n",
    "model.add(Convolution1D(nb_filter=nb_filter,\n",
    "                        filter_length=filter_length,\n",
    "                        border_mode='valid',\n",
    "                        activation='relu',\n",
    "                        subsample_length=1))\n",
    "\n",
    "def max_1d(X):\n",
    "    return K.max(X, axis=1)\n",
    "\n",
    "model.add(Lambda(max_1d, output_shape=(nb_filter,)))\n",
    "model.add(Dense(hidden_dims)) \n",
    "model.add(Dropout(0.2)) \n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "61e07990-43ef-ff27-3b36-dca146f8b4a3"
   },
   "outputs": [],
   "source": [
    "print('Train...')\n",
    "model.fit(X_train, Y_train, batch_size=32, nb_epoch=1,\n",
    "          validation_data=(X_test, Y_test))\n",
    "score, acc = model.evaluate(X_test, Y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)\n",
    "\n",
    "\n",
    "print(\"Generating test predictions...\")\n",
    "preds16 = model.predict_classes(X_test, verbose=0)\n",
    "acc16 = accuracy_score(test['Label'], preds16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "49e9abbe-d224-b757-c17b-bd4cfa5da818"
   },
   "outputs": [],
   "source": [
    "print('prediction accuracy: ', acc16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT\n",
    "\n",
    "#### Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "_cell_guid": "21b86295-1a3b-a1b8-78cf-b032b9d74eaa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "INFO:simpletransformers.classification.classification_model: Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e11a6fcc99114185b85f82573b31c302",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=2195.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae44d0b116154a50ac8b87bd71e916d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Epoch'), FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a169bcebad94ff581e2179386b2f0c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Running Epoch 0 of 2'), FloatProgress(value=0.0, max=275.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ead76369ee74c9e93900f03ce8868e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Running Epoch 1 of 2'), FloatProgress(value=0.0, max=275.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_model: Training of bert model complete. Saved to outputs/.\n",
      "INFO:simpletransformers.classification.classification_model: Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb8ce0813bb04fc69ff61ef34d91bfbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=263.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a96d260539e4b4c8f70939629d617b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=33.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__': \n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    transformers_logger = logging.getLogger(\"transformers\")\n",
    "    transformers_logger.setLevel(logging.WARNING)\n",
    "\n",
    "    # Preparing train data\n",
    "    #train_data = [\n",
    "    #    [\"Aragorn was the heir of Isildur\", 1],\n",
    "    #    [\"Frodo was the heir of Isildur\", 0],\n",
    "    #]\n",
    "    #train_df = pd.DataFrame(train)\n",
    "    #train_df.columns = [\"Tweet\", \"label\"]\n",
    "    train_df = train[[\"Tweet\", \"Label\"]]\n",
    "\n",
    "    # Preparing eval data\n",
    "    #eval_data = [\n",
    "    #    [\"Theoden was the king of Rohan\", 1],\n",
    "    #    [\"Merry was the king of Rohan\", 0],\n",
    "    #]\n",
    "    #eval_df = pd.DataFrame(test)\n",
    "    #eval_df.columns = [\"Tweet\", \"label\"]\n",
    "    eval_df = test[[\"Tweet\", \"Label\"]]\n",
    "\n",
    "    # Optional model configuration\n",
    "    model_args = ClassificationArgs(num_train_epochs=2)\n",
    "\n",
    "    model_args.overwrite_output_dir = True\n",
    "\n",
    "    # Create a ClassificationModel\n",
    "    model = ClassificationModel(\n",
    "        \"bert\", \"bert-base-cased\", args=model_args\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    model.train_model(train_df)\n",
    "\n",
    "    # Evaluate the model\n",
    "    #result, model_outputs, wrong_predictions = model.eval_model(eval_df)\n",
    "\n",
    "    # Make predictions with the model\n",
    "    X_test = test[\"Tweet\"].values.tolist()\n",
    "    Y_test = test[\"Label\"].values.tolist()\n",
    "    preds17, raw_outputs = model.predict(X_test)\n",
    "    acc17 = accuracy_score(Y_test, preds17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction accuracy:  0.5209125475285171\n"
     ]
    }
   ],
   "source": [
    "print('prediction accuracy: ', acc17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetMetrics(label,result):\n",
    "    acc1 = accuracy_score(test['Label'], preds1)\n",
    "    print('all all accuracy: ',acc1 )\n",
    "    tmp = confusion_matrix(label, result)\n",
    "    print(tmp)\n",
    "    count = len(tmp[0])\n",
    "    t1=[0,0,0,0]\n",
    "    t2=[0,0,0,0]\n",
    "    pre = [0,0,0,0]\n",
    "    recall = [0,0,0,0]\n",
    "    for i in range(count):\n",
    "        for j in range(count):\n",
    "            t1[i] += tmp[i][j]\n",
    "            t2[j] += tmp[i][j]\n",
    "    for i in range(count):\n",
    "        recall[i] = round(tmp[i][i]/t1[i],4)\n",
    "        pre[i] = round(tmp[i][i]/t2[i],4)\n",
    "    print('pre: ', pre)\n",
    "    print('recall: ', recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all all accuracy:  0.3193916349809886\n",
      "[[ 14 109]\n",
      " [ 17 123]]\n",
      "pre:  [0.4516, 0.5302, 0, 0]\n",
      "recall:  [0.1138, 0.8786, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "GetMetrics(test['Label'],preds17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "91aa05fb-b8fe-fe99-d191-c08b8dc5620c"
   },
   "source": [
    "#### Multi-Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "_cell_guid": "60fa6ce4-1658-11d2-c0ad-3b43a542fff0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "INFO:simpletransformers.classification.classification_model: Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5016523f3954dc39b9fb683d3379e3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=2195.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdd02d5701814dd49fec1840584989f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Epoch'), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e9220b0bfb342379b69bc135fb855e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Running Epoch 0 of 1'), FloatProgress(value=0.0, max=275.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_model: Training of bert model complete. Saved to outputs/.\n",
      "INFO:simpletransformers.classification.classification_model: Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cb0e815fc3d49c5abdd6d65f75022b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=263.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "413a76776afb4da997fc3c231a4698dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=33.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__': \n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    transformers_logger = logging.getLogger(\"transformers\")\n",
    "    transformers_logger.setLevel(logging.WARNING)\n",
    "\n",
    "    # Preparing train data\n",
    "    train_mf = train[[\"Tweet\", \"Label_4part\"]]\n",
    "\n",
    "    # Preparing eval data\n",
    "    eval_mf = test[[\"Tweet\", \"Label_4part\"]]\n",
    "\n",
    "    # Optional model configuration\n",
    "    model_args = ClassificationArgs(num_train_epochs=1)\n",
    "\n",
    "    model_args.overwrite_output_dir = True\n",
    "\n",
    "    # Create a ClassificationModel\n",
    "    model = ClassificationModel(\n",
    "        'bert',\n",
    "        'bert-base-cased',\n",
    "        num_labels=4,\n",
    "        args=model_args,\n",
    "        use_cuda=False\n",
    "    ) \n",
    "\n",
    "    # Train the model\n",
    "    model.train_model(train_df)\n",
    "\n",
    "    # Evaluate the model\n",
    "    #result, model_outputs, wrong_predictions = model.eval_model(eval_df)\n",
    "    \n",
    "    # Make predictions with the model\n",
    "    X_test = test[\"Tweet\"].values.tolist()\n",
    "    Y_test = test[\"Label_4part\"].values.tolist()\n",
    "    preds18, raw_outputs = model.predict(X_test)\n",
    "    acc18 = accuracy_score(Y_test, preds18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "_cell_guid": "03273578-b025-4e87-6088-a993fce31e2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction accuracy:  0.3231939163498099\n"
     ]
    }
   ],
   "source": [
    "print('prediction accuracy: ', acc18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetMetrics(label,result):\n",
    "    acc1 = accuracy_score(test['Label_4part'], preds1)\n",
    "    print('all all accuracy: ',acc1 )\n",
    "    tmp = confusion_matrix(label, result)\n",
    "    print(tmp)\n",
    "    count = len(tmp[0])\n",
    "    t1=[0,0,0,0]\n",
    "    t2=[0,0,0,0]\n",
    "    pre = [0,0,0,0]\n",
    "    recall = [0,0,0,0]\n",
    "    for i in range(count):\n",
    "        for j in range(count):\n",
    "            t1[i] += tmp[i][j]\n",
    "            t2[j] += tmp[i][j]\n",
    "    for i in range(count):\n",
    "        recall[i] = round(tmp[i][i]/t1[i],4)\n",
    "        pre[i] = round(tmp[i][i]/t2[i],4)\n",
    "    print('pre: ', pre)\n",
    "    print('recall: ', recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all all accuracy:  0.33079847908745247\n",
      "[[  0  38   0   0]\n",
      " [  0  85   0   0]\n",
      " [  0 113   0   0]\n",
      " [  0  27   0   0]]\n",
      "pre:  [nan, 0.3232, nan, nan]\n",
      "recall:  [0.0, 1.0, 0.0, 0.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-120-e19f7a2df261>:17: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  pre[i] = round(tmp[i][i]/t2[i],4)\n"
     ]
    }
   ],
   "source": [
    "GetMetrics(test['Label_4part'],preds18)"
   ]
  }
 ],
 "metadata": {
  "_change_revision": 1261,
  "_is_fork": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
